import logging
import uuid
from datetime import datetime, timezone
from app.schemas.log_schema import AnalysisLog, LogMeta, SessionInfo, Context, UserInput, Evaluation, LogFeedback, DebugInfo
from app.db.firestore import get_db

logger = logging.getLogger(__name__)

class LoggingService:
    def __init__(self):
        self.db = get_db()
        self.collection_name = "analysis_logs_v2" # 새로운 스키마 적용

    def save_log(self, log_data: AnalysisLog) -> str:
        if not self.db:
            logger.warning("Firestore DB not available, skipping log save.")
            return None
        
        try:
            doc_ref = self.db.collection(self.collection_name).document(log_data.meta.log_id)
            doc_ref.set(log_data.model_dump())
            logger.info(f"✅ Log saved: {log_data.meta.log_id}")
            return log_data.meta.log_id
        except Exception as e:
            logger.error(f"❌ Failed to save log: {e}")
            return None

    def create_log_entry(self, 
                         user_id: str, 
                         session_id: str, 
                         text: str, 
                         evaluation_result: dict,
                         feedback_msg: str) -> AnalysisLog:
        """
        간소화된 입력으로 전체 로그 객체를 생성하는 헬퍼 함수
        (실제로는 evaluation_service에서 더 자세한 데이터를 넘겨받아야 완성도 높은 로그가 됨)
        """
        now = datetime.now(timezone.utc)
        log_id = f"log_{now.strftime('%Y%m%d')}_{user_id}_{uuid.uuid4().hex[:6]}"
        
        # Meta
        meta = LogMeta(log_id=log_id, timestamp=now)
        
        # Session (임시 데이터)
        session = SessionInfo(session_id=session_id, user_id=user_id, passage_id="unknown")
        
        # Context (임시 데이터)
        context = Context(
            step_index=1, 
            step_code="S1_ANALYSIS", 
            step_name_kr="분석", 
            attempt={"current": 1, "max_allowed": 3}
        )
        
        # User Input
        user_input = UserInput(
            text=text,
            length={"chars": len(text), "tokens": len(text.split())},
            time_spent_sec=0.0,
            interaction_flags={}
        )
        
        # Evaluation (Mapping from evaluator result)
        model_eval_scores = {
            "qa_score": evaluation_result.get("final_score", 0.0), # 임시 매핑
            "link_score": evaluation_result.get("sts_score", 0.0),
            "length_chars": len(text),
            "length_tokens": len(text.split()),
            "evidence_refs": 0
        }
        
        model_eval = {
            "diag_code": "N/A",
            "scores": model_eval_scores
        }
        
        rule_eval = None # 필요시 추가 구현
        
        evaluation = Evaluation(
            final_status="PASS" if evaluation_result.get("is_passed") else "RETRY",
            label="SUCCESS" if evaluation_result.get("is_passed") else "WEAK_LINK",
            model_eval=model_eval,
            rule_eval=rule_eval
        )
        
        # Feedback
        feedback = LogFeedback(
            message_shown=feedback_msg,
            ui_actions=[]
        )
        
        # Debug
        debug = DebugInfo(
            evaluator_rule="basic_evaluator",
            model_version="v1.0",
            notes="Generated by logging_service"
        )
        
        return AnalysisLog(
            meta=meta,
            session=session,
            context=context,
            user_input=user_input,
            evaluation=evaluation,
            feedback=feedback,
            debug=debug
        )
