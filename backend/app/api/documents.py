"""
ë¬¸ì„œ ì—…ë¡œë“œ API
ì—­í• : í•™ìŠµ ë¬¸ì„œ(PDF/TXT/DOCX) ì—…ë¡œë“œ, í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì²­í¬ ë¶„í• 

ğŸ“‹ í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œìë¥¼ ìœ„í•œ ì‚¬ìš© ê°€ì´ë“œ
==========================================

1. ë¬¸ì„œ ì—…ë¡œë“œ: POST /documents
   - multipart/form-dataë¡œ íŒŒì¼ ì „ì†¡
   - ì‘ë‹µìœ¼ë¡œ document_id ë°›ìŒ
   - âœ¨ Google Document AIë¡œ ê³ í’ˆì§ˆ OCR ì²˜ë¦¬

2. ìƒíƒœ í™•ì¸: GET /documents/{document_id}
   - statusê°€ "ready"ê°€ ë  ë•Œê¹Œì§€ í´ë§

3. í”„ë¦¬ë·° í™•ì¸: GET /documents/{document_id}/preview
   - ì—…ë¡œë“œ í™•ì¸ ëª¨ë‹¬ì— í‘œì‹œí•  í…ìŠ¤íŠ¸

4. ì²­í¬ ì¡°íšŒ: GET /documents/{document_id}/chunks
   - ì„¸ì…˜ ì‹œì‘ ì‹œ chunk_id ì„ íƒìš©
"""

from fastapi import APIRouter, HTTPException, status, UploadFile, File, Form, Depends
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid
import os
import tempfile

# from sqlalchemy.ext.asyncio import AsyncSession  # Removed
# from sqlalchemy import select  # Removed
# from app.db.session import get_db  # Removed
# from app.db.models import User, RAGDocument, TextChunk  # Removed

from app.schemas.user import User
from app.schemas.document import RAGDocumentCreate
from app.schemas.work import TextChunkCreate
from app.repository.document_repository import DocumentRepository
from app.repository.work_repository import WorkRepository
from app.core.auth import get_current_user
from app.services.document_ai import get_document_ai_service

# Helper function
def split_into_chunks(text: str, chunk_size: int = 500) -> List[Dict]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„ë¦¬
    paragraphs = text.split("\n\n")
    
    current_chunk = ""
    current_start = 0
    sequence = 1
    
    for para in paragraphs:
        if len(current_chunk) + len(para) > chunk_size and current_chunk:
            chunks.append({
                "sequence": sequence,
                "text": current_chunk.strip(),
                "anchor": {
                    "char_start": current_start,
                    "char_end": current_start + len(current_chunk),
                    "paragraph": sequence
                }
            })
            sequence += 1
            current_start += len(current_chunk)
            current_chunk = para + "\n\n"
        else:
            current_chunk += para + "\n\n"
    
    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk.strip():
        chunks.append({
            "sequence": sequence,
            "text": current_chunk.strip(),
            "anchor": {
                "char_start": current_start,
                "char_end": current_start + len(current_chunk),
                "paragraph": sequence
            }
        })
    
    return chunks

router = APIRouter(prefix="/documents", tags=["ğŸ“„ Document Management"])


# ============================================================
# Request/Response Models - í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì ì°¸ê³ ìš© ìƒì„¸ ì„¤ëª…
# ============================================================

class DocumentUploadResponse(BaseModel):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ì‘ë‹µ
    """
    document_id: str = Field(..., description="ë¬¸ì„œ ê³ ìœ  ID. ì´í›„ ëª¨ë“  API í˜¸ì¶œì— ì‚¬ìš©", example="doc_abc123")
    status: str = Field(..., description="ì²˜ë¦¬ ìƒíƒœ: processing(ì²˜ë¦¬ì¤‘) | ready(ì™„ë£Œ) | failed(ì‹¤íŒ¨)", example="processing")
    message: str = Field(..., description="ì‚¬ìš©ìì—ê²Œ í‘œì‹œí•  ë©”ì‹œì§€", example="ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    meta: Dict[str, Any] = Field(default_factory=dict, description="íŒŒì¼ ë©”íƒ€ë°ì´í„°")


class DocumentStatusResponse(BaseModel):
    """
    ë¬¸ì„œ ìƒíƒœ ì¡°íšŒ ì‘ë‹µ
    """
    document_id: str = Field(..., description="ë¬¸ì„œ ê³ ìœ  ID")
    status: str = Field(..., description="ì²˜ë¦¬ ìƒíƒœ", example="ready")
    title: Optional[str] = Field(None, description="ë¬¸ì„œ ì œëª©")
    total_chunks: int = Field(0, description="ë¶„í• ëœ ì²­í¬ ìˆ˜")
    total_chars: int = Field(0, description="ì´ ë¬¸ì ìˆ˜")
    created_at: str = Field(..., description="ìƒì„± ì‹œê° (ISO 8601)")
    error_message: Optional[str] = Field(None, description="ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€")


class DocumentPreviewResponse(BaseModel):
    """
    ë¬¸ì„œ í”„ë¦¬ë·° ì‘ë‹µ - ì—…ë¡œë“œ í™•ì¸ ëª¨ë‹¬ìš©
    """
    document_id: str
    title: Optional[str]
    preview: Dict[str, str] = Field(..., description="ì•/ì¤‘ê°„/ë’·ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸°")
    total_chars: int


class ChunkItem(BaseModel):
    """
    ì²­í¬ ì •ë³´ - ì„¸ì…˜ ì‹œì‘ ì‹œ chunk_id ì„ íƒì— ì‚¬ìš©
    """
    chunk_id: str = Field(..., description="ì²­í¬ ê³ ìœ  ID", example="chunk_001")
    sequence: int = Field(..., description="ìˆœì„œ (1ë¶€í„° ì‹œì‘)", example=1)
    text: str = Field(..., description="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©")
    anchor: Dict[str, Any] = Field(..., description="ìœ„ì¹˜ ì •ë³´ {page, paragraph, char_start, char_end}")


class DocumentChunksResponse(BaseModel):
    """
    ë¬¸ì„œ ì²­í¬ ëª©ë¡ ì‘ë‹µ
    """
    document_id: str
    chunks: List[ChunkItem]
    total_chunks: int


# ============================================================
# API Endpoints
# ============================================================

@router.post(
    "",
    response_model=DocumentUploadResponse,
    summary="ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ",
    description="ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤."
)
async def upload_document(
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  ë¬¸ì„œ íŒŒì¼ (PDF/TXT/DOCX)"),
    title: Optional[str] = Form(None, description="ë¬¸ì„œ ì œëª© (ìƒëµ ì‹œ íŒŒì¼ëª… ì‚¬ìš©)"),
    current_user: User = Depends(get_current_user)
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    """
    
    # íŒŒì¼ í¬ê¸° ì²´í¬ (10MB)
    MAX_SIZE = 10 * 1024 * 1024
    content = await file.read()
    if len(content) > MAX_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail="íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤"
        )
    
    # ì„ì‹œ íŒŒì¼ ì €ì¥ (Cloud Runì—ì„œëŠ” /tmp ì‚¬ìš©)
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, f"{uuid.uuid4()}_{file.filename}")
    
    try:
        with open(temp_path, "wb") as f:
            f.write(content)
        
        # Document AI ì„œë¹„ìŠ¤ë¥¼ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        doc_service = get_document_ai_service()
        result = await doc_service.process_document(
            file_path=temp_path,
            mime_type=file.content_type or "application/pdf"
        )
        extracted_text = result["text"]
        
        # ì²­í¬ ë¶„í• 
        chunks = split_into_chunks(extracted_text)
        
        # ë¬¸ì„œ ID ìƒì„±
        doc_id = f"doc_{uuid.uuid4().hex[:12]}"
        doc_title = title or os.path.splitext(file.filename)[0]
        
        # RAGDocument ì €ì¥ (Firestore)
        doc_repo = DocumentRepository()
        doc_create = RAGDocumentCreate(
            doc_id=doc_id,
            doc_type="uploaded",
            content=extracted_text,
            usage_stages=["LEARNING"],
            priority=5
        )
        await doc_repo.create_document(doc_create)
        
        # TextChunk ì €ì¥ (Firestore)
        work_repo = WorkRepository()
        for chunk_data in chunks:
            chunk_create = TextChunkCreate(
                chunk_id=f"{doc_id}_chunk_{chunk_data['sequence']:03d}",
                work_id=doc_id,  # ë¬¸ì„œ IDë¥¼ work_idë¡œ ì‚¬ìš©
                sequence=chunk_data["sequence"],
                chunk_type="paragraph",
                content=chunk_data["text"],
                tags=chunk_data["anchor"]
            )
            await work_repo.create_chunk(chunk_create)
        
        return DocumentUploadResponse(
            document_id=doc_id,
            status="ready",
            message="ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            meta={
                "filename": file.filename,
                "file_size": len(content),
                "mime_type": file.content_type,
                "total_chars": len(extracted_text),
                "total_chunks": len(chunks),
                "source": result.get("source", "unknown")
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        )
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(temp_path):
            os.remove(temp_path)


@router.get(
    "/{document_id}",
    response_model=DocumentStatusResponse,
    summary="ğŸ“‹ ë¬¸ì„œ ìƒíƒœ ì¡°íšŒ"
)
async def get_document_status(
    document_id: str,
    current_user: User = Depends(get_current_user)
):
    """ë¬¸ì„œ ìƒíƒœ ì¡°íšŒ"""
    
    doc_repo = DocumentRepository()
    doc = await doc_repo.get_document(document_id)
    
    if not doc:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
        )
    
    # ì²­í¬ ìˆ˜ ì¡°íšŒ (Firestore)
    work_repo = WorkRepository()
    chunks = await work_repo.get_chunks_by_work(document_id)
    
    return DocumentStatusResponse(
        document_id=document_id,
        status="ready",
        title=document_id,  # ì‹¤ì œë¡œëŠ” ë³„ë„ title í•„ë“œ í•„ìš”í•˜ì§€ë§Œ ìŠ¤í‚¤ë§ˆì— ì—†ìŒ
        total_chunks=len(chunks),
        total_chars=len(doc.content) if doc.content else 0,
        created_at=doc.created_at
    )


@router.get(
    "/{document_id}/preview",
    response_model=DocumentPreviewResponse,
    summary="ğŸ‘ï¸ ë¬¸ì„œ í”„ë¦¬ë·° ì¡°íšŒ"
)
async def get_document_preview(
    document_id: str,
    current_user: User = Depends(get_current_user)
):
    """ë¬¸ì„œ í”„ë¦¬ë·° ì¡°íšŒ"""
    
    doc_repo = DocumentRepository()
    doc = await doc_repo.get_document(document_id)
    
    if not doc:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
        )
    
    content = doc.content or ""
    total_len = len(content)
    
    # ì•/ì¤‘ê°„/ë’¤ ê° 200ì
    preview_len = 200
    
    preview = {
        "beginning": content[:preview_len] + ("..." if total_len > preview_len else ""),
        "middle": "",
        "end": ""
    }
    
    if total_len > preview_len * 2:
        mid_start = (total_len - preview_len) // 2
        preview["middle"] = "..." + content[mid_start:mid_start + preview_len] + "..."
    
    if total_len > preview_len:
        preview["end"] = "..." + content[-preview_len:]
    
    return DocumentPreviewResponse(
        document_id=document_id,
        title=document_id,
        preview=preview,
        total_chars=total_len
    )


@router.get(
    "/{document_id}/chunks",
    response_model=DocumentChunksResponse,
    summary="ğŸ“‘ ë¬¸ì„œ ì²­í¬ ëª©ë¡ ì¡°íšŒ"
)
async def get_document_chunks(
    document_id: str,
    current_user: User = Depends(get_current_user)
):
    """ë¬¸ì„œ ì²­í¬ ëª©ë¡ ì¡°íšŒ"""
    
    work_repo = WorkRepository()
    chunks = await work_repo.get_chunks_by_work(document_id)
    
    if not chunks:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"ë¬¸ì„œ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}"
        )
    
    chunk_items = [
        ChunkItem(
            chunk_id=chunk.chunk_id,
            sequence=chunk.sequence,
            text=chunk.content,
            anchor=chunk.tags if chunk.tags else {"paragraph": chunk.sequence}
        )
        for chunk in chunks
    ]
    
    return DocumentChunksResponse(
        document_id=document_id,
        chunks=chunk_items,
        total_chunks=len(chunk_items)
    )


@router.get(
    "",
    summary="ğŸ“š ë‚´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"
)
async def list_documents(
    current_user: User = Depends(get_current_user)
):
    """ë‚´ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    
    doc_repo = DocumentRepository()
    # í˜„ì¬ëŠ” ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
    docs = await doc_repo.get_documents_by_type("uploaded")
    
    return {
        "documents": [
            {
                "document_id": doc.doc_id,
                "title": doc.doc_id,
                "status": "ready",
                "total_chars": len(doc.content) if doc.content else 0,
                "created_at": doc.created_at
            }
            for doc in docs
        ],
        "total": len(docs)
    }
