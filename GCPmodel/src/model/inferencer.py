"""
ì‚¬ê³ ìœ ë„ ì¶”ë¡  ì—”ì§„
íŒŒì¸íŠœë‹ëœ Gemma ëª¨ë¸ë¡œ [ì‚¬ê³ ìœ ë„] + [ì‚¬ê³ ë¡œê·¸] ìƒì„±
"""

import re
import json
from pathlib import Path
from typing import Dict, Optional, List
from datetime import datetime

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel


class ThoughtInducer:
    """ì‚¬ê³ ìœ ë„ ì¶”ë¡  ì—”ì§„"""

    def __init__(
        self,
        model_path: str = "",  # TODO: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
        base_model_name: str = "google/gemma-3-9b-it",
        load_in_4bit: bool = True,
        device: Optional[str] = None
    ):
        """
        Args:
            model_path: íŒŒì¸íŠœë‹ëœ LoRA ëª¨ë¸ ê²½ë¡œ (ë¹„ì–´ìˆìœ¼ë©´ base ëª¨ë¸ë§Œ ì‚¬ìš©)
            base_model_name: ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„
            load_in_4bit: 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        """
        self.model_path = model_path
        self.base_model_name = base_model_name
        self.load_in_4bit = load_in_4bit
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        self.model = None
        self.tokenizer = None
        self.is_loaded = False

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """í•™ìƒì˜ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ë©° ê³ ì „ë¬¸í•™ì„ ê°€ë¥´ì¹˜ì„¸ìš”. [ì‚¬ê³ ìœ ë„]ì™€ [ì‚¬ê³ ë¡œê·¸] íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

[ì‚¬ê³ ìœ ë„]: í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ì  ì§ˆë¬¸ì„ ì œì‹œí•©ë‹ˆë‹¤.
[ì‚¬ê³ ë¡œê·¸]: í•™ìƒì˜ ì‚¬ê³  ê³¼ì •ì„ ê´€ì°°í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤."""

    def load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        if self.is_loaded:
            print("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")

        # Tokenizer ë¡œë“œ
        tokenizer_path = self.model_path if self.model_path else self.base_model_name
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True
        )
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # ì–‘ìí™” ì„¤ì •
        if self.load_in_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        else:
            bnb_config = None

        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )

        # LoRA ì–´ëŒ‘í„° ë¡œë“œ (íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
        if self.model_path and Path(self.model_path).exists():
            print(f"   LoRA ì–´ëŒ‘í„° ë¡œë“œ: {self.model_path}")
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
        else:
            print(f"   âš ï¸ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì—†ìŒ. ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©.")
            self.model = base_model

        self.model.eval()
        self.is_loaded = True
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def generate_response(
        self,
        student_input: str,
        context: Optional[str] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> Dict:
        """
        ì‚¬ê³ ìœ ë„ ì‘ë‹µ ìƒì„±

        Args:
            student_input: í•™ìƒ ì§ˆë¬¸/ì…ë ¥
            context: ì¶”ê°€ ë§¥ë½ (ì‘í’ˆëª…, ì§€ë¬¸ ë“±)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒ˜í”Œë§ ì˜¨ë„
            top_p: top-p ìƒ˜í”Œë§
            do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€

        Returns:
            Dict: {
                "induction": ì‚¬ê³ ìœ ë„ ë‚´ìš©,
                "log": ì‚¬ê³ ë¡œê·¸ ë‚´ìš©,
                "full_response": ì „ì²´ ì‘ë‹µ,
                "metadata": ë©”íƒ€ë°ì´í„°
            }
        """
        if not self.is_loaded:
            self.load_model()

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context:
            prompt = f"""{self.system_prompt}

[ë§¥ë½]
{context}

í•™ìƒ: {student_input}

AI: [ì‚¬ê³ ìœ ë„]"""
        else:
            prompt = f"""{self.system_prompt}

í•™ìƒ: {student_input}

AI: [ì‚¬ê³ ìœ ë„]"""

        # í† í°í™”
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                top_p=top_p,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # AI ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        ai_response = full_response.split("AI: ")[-1] if "AI: " in full_response else full_response

        # íƒœê·¸ ì¶”ì¶œ
        induction = self._extract_tag(ai_response, "ì‚¬ê³ ìœ ë„")
        log = self._extract_tag(ai_response, "ì‚¬ê³ ë¡œê·¸")

        return {
            "induction": induction,
            "log": log,
            "full_response": ai_response,
            "metadata": {
                "student_input": student_input,
                "context": context,
                "timestamp": datetime.now().isoformat(),
                "model_path": self.model_path or self.base_model_name
            }
        }

    def _extract_tag(self, text: str, tag: str) -> str:
        """íƒœê·¸ ë‚´ìš© ì¶”ì¶œ"""
        # [íƒœê·¸] ë‚´ìš© ì¶”ì¶œ íŒ¨í„´
        pattern = rf"\[{tag}\]\s*(.*?)(?=\[|$)"
        match = re.search(pattern, text, re.DOTALL)

        if match:
            return match.group(1).strip()
        return ""

    def chat(
        self,
        messages: List[Dict[str, str]],
        max_new_tokens: int = 512
    ) -> Dict:
        """
        ë©€í‹°í„´ ëŒ€í™” ì§€ì›

        Args:
            messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "student/ai", "content": "..."}]
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜

        Returns:
            Dict: ìƒì„±ëœ ì‘ë‹µ
        """
        if not self.is_loaded:
            self.load_model()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        conversation = f"{self.system_prompt}\n\n"

        for msg in messages:
            role = msg.get("role", "student")
            content = msg.get("content", "")

            if role == "student":
                conversation += f"í•™ìƒ: {content}\n\n"
            else:
                conversation += f"AI: {content}\n\n"

        # ë§ˆì§€ë§‰ì´ í•™ìƒ ë©”ì‹œì§€ë©´ AI ì‘ë‹µ ìƒì„±
        if messages and messages[-1].get("role") == "student":
            conversation += "AI: [ì‚¬ê³ ìœ ë„]"

        # í† í°í™” ë° ìƒì„±
        inputs = self.tokenizer(
            conversation,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )

        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        ai_response = full_response.split("AI: ")[-1] if "AI: " in full_response else ""

        # ë‹¤ìŒ í•™ìƒ ì‘ë‹µ ì´ì „ê¹Œì§€ë§Œ ì¶”ì¶œ
        if "í•™ìƒ:" in ai_response:
            ai_response = ai_response.split("í•™ìƒ:")[0].strip()

        return {
            "induction": self._extract_tag(ai_response, "ì‚¬ê³ ìœ ë„"),
            "log": self._extract_tag(ai_response, "ì‚¬ê³ ë¡œê·¸"),
            "full_response": ai_response,
            "conversation_length": len(messages) + 1
        }

    def save_thought_log(
        self,
        response: Dict,
        output_dir: str = "outputs/thought_logs"
    ) -> str:
        """
        ì‚¬ê³ ë¡œê·¸ ì €ì¥

        Args:
            response: generate_response ê²°ê³¼
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"thought_log_{timestamp}.json"
        filepath = output_dir / filename

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(response, f, ensure_ascii=False, indent=2)

        return str(filepath)

    def batch_generate(
        self,
        inputs: List[str],
        contexts: Optional[List[str]] = None,
        output_path: Optional[str] = None
    ) -> List[Dict]:
        """
        ë°°ì¹˜ ìƒì„±

        Args:
            inputs: í•™ìƒ ì…ë ¥ ë¦¬ìŠ¤íŠ¸
            contexts: ë§¥ë½ ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒ)

        Returns:
            List[Dict]: ìƒì„± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        from tqdm import tqdm

        if not self.is_loaded:
            self.load_model()

        results = []
        contexts = contexts or [None] * len(inputs)

        for student_input, context in tqdm(zip(inputs, contexts), total=len(inputs), desc="ìƒì„± ì¤‘"):
            result = self.generate_response(student_input, context)
            results.append(result)

        # ì €ì¥
        if output_path:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                for item in results:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')

            print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

        return results


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆì‹œ
    inducer = ThoughtInducer(
        model_path="",  # TODO: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
        base_model_name="google/gemma-3-9b-it"
    )

    # í…ŒìŠ¤íŠ¸ (ëª¨ë¸ ë¡œë“œ í•„ìš”)
    # inducer.load_model()
    # result = inducer.generate_response(
    #     student_input="ì¶˜í–¥ì „ì—ì„œ ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ê°€ ë­”ê°€ìš”?",
    #     context="ì¶˜í–¥ì „"
    # )
    # print(result)

    print("ì¶”ë¡  ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ. model_pathë¥¼ ì„¤ì •í•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”.")
