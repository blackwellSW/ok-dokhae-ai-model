"""
Gemma 3 íŒŒì¸íŠœë‹ ëª¨ë“ˆ
LoRA ê¸°ë°˜ íš¨ìœ¨ì  í•™ìŠµ
"""

import os
import json
from pathlib import Path
from typing import Dict, Optional, List
from dataclasses import dataclass, field

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import load_dataset, Dataset


@dataclass
class TrainingConfig:
    """í•™ìŠµ ì„¤ì • ë°ì´í„° í´ë˜ìŠ¤"""
    # Model
    model_name: str = "google/gemma-3-9b-it"
    load_in_4bit: bool = True
    torch_dtype: str = "bfloat16"

    # LoRA
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: List[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"]
    )

    # Training
    output_dir: str = "models/gemma_finetuned"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    max_length: int = 1024
    warmup_steps: int = 100
    logging_steps: int = 10
    save_steps: int = 100

    # Data paths (TODO: ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½)
    train_data_path: str = ""  # TODO: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
    valid_data_path: str = ""  # TODO: ê²€ì¦ ë°ì´í„° ê²½ë¡œ


class GemmaTrainer:
    """Gemma 3 íŒŒì¸íŠœë‹ í´ë˜ìŠ¤"""

    def __init__(self, config: Optional[TrainingConfig] = None):
        """
        Args:
            config: í•™ìŠµ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.config = config or TrainingConfig()
        self.model = None
        self.tokenizer = None

        # LoRA ì„¤ì •
        self.lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM
        )

    def load_model(self) -> tuple:
        """
        4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë“œ

        Returns:
            tuple: (model, tokenizer)
        """
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”©: {self.config.model_name}")

        # Tokenizer ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True
        )
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "right"

        # 4-bit ì–‘ìí™” ì„¤ì •
        if self.config.load_in_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=getattr(torch, self.config.torch_dtype),
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        else:
            bnb_config = None

        # ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=getattr(torch, self.config.torch_dtype),
            trust_remote_code=True
        )

        # LoRAë¥¼ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
        if self.config.load_in_4bit:
            model = prepare_model_for_kbit_training(model)

        # LoRA ì ìš©
        model = get_peft_model(model, self.lora_config)

        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")

        self.model = model
        self.tokenizer = tokenizer

        return model, tokenizer

    def prepare_dataset(self, data_path: str) -> Dataset:
        """
        ë°ì´í„°ì…‹ ì¤€ë¹„

        Args:
            data_path: JSONL ë°ì´í„° íŒŒì¼ ê²½ë¡œ

        Returns:
            Dataset: í† í°í™”ëœ ë°ì´í„°ì…‹
        """
        if not data_path or not Path(data_path).exists():
            print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            print("   í•™ìŠµ ë°ì´í„° ê²½ë¡œë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return None

        # ë°ì´í„° ë¡œë“œ
        dataset = load_dataset('json', data_files=data_path, split='train')

        def format_and_tokenize(example):
            """ë°ì´í„° í¬ë§·íŒ… ë° í† í°í™”"""
            # í•™ìŠµ í…ìŠ¤íŠ¸ êµ¬ì„±
            if all(k in example for k in ("instruction", "input", "output")):
                instruction = str(example.get("instruction", ""))
                user_input = str(example.get("input", ""))
                output = str(example.get("output", ""))
            else:
                # raw í˜•ì‹(id/source/passage/question/answer)ë„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì§€ì›
                instruction = "í•™ìƒì˜ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ë©° ì‘í’ˆì„ í•´ì„¤í•˜ì„¸ìš”."
                passage = str(example.get("passage", ""))
                question = str(example.get("question", ""))
                answer = str(example.get("answer", ""))
                user_input = f"[ì§€ë¬¸]\n{passage}\n\n[ì§ˆë¬¸]\n{question}"
                output = answer

            text = f"""{instruction}

{user_input}

{output}"""

            # í† í°í™”
            tokens = self.tokenizer(
                text,
                truncation=True,
                max_length=self.config.max_length,
                padding="max_length"
            )
            tokens["labels"] = tokens["input_ids"].copy()

            return tokens

        # í† í°í™” ì ìš©
        tokenized_dataset = dataset.map(
            format_and_tokenize,
            remove_columns=dataset.column_names
        )

        print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {len(tokenized_dataset)}ê°œ")
        return tokenized_dataset

    def train(
        self,
        train_dataset: Optional[Dataset] = None,
        eval_dataset: Optional[Dataset] = None,
        resume_from_checkpoint: Optional[str] = None
    ) -> str:
        """
        í•™ìŠµ ì‹¤í–‰

        Args:
            train_dataset: í•™ìŠµ ë°ì´í„°ì…‹ (Noneì´ë©´ config ê²½ë¡œì—ì„œ ë¡œë“œ)
            eval_dataset: í‰ê°€ ë°ì´í„°ì…‹ (ì„ íƒ)
            resume_from_checkpoint: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘

        Returns:
            str: ì €ì¥ëœ ëª¨ë¸ ê²½ë¡œ
        """
        # ëª¨ë¸ ë¡œë“œ
        if self.model is None:
            self.load_model()

        # ë°ì´í„°ì…‹ ì¤€ë¹„
        if train_dataset is None:
            if not self.config.train_data_path:
                print("âš ï¸ í•™ìŠµ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("   config.train_data_pathë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                return None
            train_dataset = self.prepare_dataset(self.config.train_data_path)

        if train_dataset is None:
            print("âŒ í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        if eval_dataset is None and self.config.valid_data_path:
            eval_dataset = self.prepare_dataset(self.config.valid_data_path)

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            fp16=True,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            warmup_steps=self.config.warmup_steps,
            optim="paged_adamw_8bit",
            save_total_limit=3,
            report_to="none",  # wandb ë“± ë¹„í™œì„±í™”
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=self.config.save_steps if eval_dataset else None,
        )

        # Data Collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )

        # Trainer ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator
        )

        print("=" * 60)
        print("ğŸš€ í•™ìŠµ ì‹œì‘")
        print("=" * 60)
        print(f"   ëª¨ë¸: {self.config.model_name}")
        print(f"   ë°ì´í„°: {len(train_dataset)}ê°œ")
        print(f"   Epochs: {self.config.num_train_epochs}")
        print(f"   Batch size: {self.config.per_device_train_batch_size}")
        print(f"   Learning rate: {self.config.learning_rate}")
        print("=" * 60)

        # í•™ìŠµ ì‹¤í–‰
        trainer.train(resume_from_checkpoint=resume_from_checkpoint)

        # ëª¨ë¸ ì €ì¥
        output_path = Path(self.config.output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        trainer.save_model(str(output_path))
        self.tokenizer.save_pretrained(str(output_path))

        # ì„¤ì • ì €ì¥
        config_path = output_path / "training_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config.__dict__, f, ensure_ascii=False, indent=2)

        print("=" * 60)
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: {output_path}")
        print("=" * 60)

        return str(output_path)

    @classmethod
    def from_yaml(cls, config_path: str) -> "GemmaTrainer":
        """
        YAML ì„¤ì • íŒŒì¼ì—ì„œ Trainer ìƒì„±

        Args:
            config_path: YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ

        Returns:
            GemmaTrainer: ì„¤ì •ì´ ì ìš©ëœ Trainer
        """
        import yaml

        with open(config_path, 'r', encoding='utf-8') as f:
            yaml_config = yaml.safe_load(f)

        config = TrainingConfig(
            # Model settings
            model_name=yaml_config.get('model', {}).get('name', TrainingConfig.model_name),
            load_in_4bit=yaml_config.get('model', {}).get('load_in_4bit', True),
            torch_dtype=yaml_config.get('model', {}).get('torch_dtype', 'bfloat16'),

            # LoRA settings
            lora_r=yaml_config.get('lora', {}).get('r', 16),
            lora_alpha=yaml_config.get('lora', {}).get('lora_alpha', 32),
            lora_dropout=yaml_config.get('lora', {}).get('lora_dropout', 0.05),
            target_modules=yaml_config.get('lora', {}).get('target_modules', ["q_proj", "k_proj", "v_proj", "o_proj"]),

            # Training settings
            output_dir=yaml_config.get('training', {}).get('output_dir', 'models/gemma_finetuned'),
            num_train_epochs=yaml_config.get('training', {}).get('num_train_epochs', 3),
            per_device_train_batch_size=yaml_config.get('training', {}).get('per_device_train_batch_size', 4),
            gradient_accumulation_steps=yaml_config.get('training', {}).get('gradient_accumulation_steps', 4),
            learning_rate=yaml_config.get('training', {}).get('learning_rate', 2e-4),
            max_length=yaml_config.get('training', {}).get('max_length', 1024),

            # Data paths
            train_data_path=yaml_config.get('data', {}).get('train_data_path', ''),
            valid_data_path=yaml_config.get('data', {}).get('valid_data_path', '')
        )

        return cls(config)


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # ë°©ë²• 1: ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
    # trainer = GemmaTrainer()

    # ë°©ë²• 2: YAML ì„¤ì • íŒŒì¼ë¡œ ì‹¤í–‰
    # trainer = GemmaTrainer.from_yaml("configs/training_config.yaml")

    # ë°©ë²• 3: ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
    config = TrainingConfig(
        model_name="google/gemma-3-9b-it",
        train_data_path="data/processed/train.jsonl",  # TODO: ì‹¤ì œ ê²½ë¡œ
        valid_data_path="data/processed/valid.jsonl",  # TODO: ì‹¤ì œ ê²½ë¡œ
        num_train_epochs=3,
        output_dir="models/gemma_finetuned"
    )
    trainer = GemmaTrainer(config)

    # í•™ìŠµ ì‹¤í–‰
    # model_path = trainer.train()
    print("í•™ìŠµ ì¤€ë¹„ ì™„ë£Œ. train_data_pathë¥¼ ì„¤ì •í•˜ê³  trainer.train()ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
