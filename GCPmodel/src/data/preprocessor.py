"""
ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
AI HUB ë°ì´í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
"""

import json
import os
import zipfile
from pathlib import Path
from typing import List, Dict, Any, Iterable, Union
from tqdm import tqdm


class DataPreprocessor:
    """AI HUB ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, config_path: str = "configs/training_config.yaml"):
        """
        Args:
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        """
        self.config = self._load_config(config_path)

        self.raw_classics_path = self.config.get("data", {}).get("raw_classics_path", "")
        self.raw_comprehension_path = self.config.get("data", {}).get("raw_comprehension_path", "")
        self.raw_evaluation_path = self.config.get("data", {}).get("raw_evaluation_path", "")

        self.output_dir = Path("data/processed")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _load_config(self, config_path: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"Config file not found: {config_path}")
            return {}
        except Exception as e:
            print(f"Error loading config: {e}")
            return {}

    def _normalize_paths(self, raw_path: Union[str, List[str]]) -> List[Path]:
        """ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ê²½ë¡œë¥¼ Path ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”"""
        if isinstance(raw_path, list):
            items = raw_path
        elif isinstance(raw_path, str) and raw_path.strip():
            items = [raw_path]
        else:
            return []

        resolved = []
        for item in items:
            if not isinstance(item, str) or not item.strip():
                continue
            p = Path(os.path.expanduser(os.path.expandvars(item.strip())))
            resolved.append(p)
        return resolved

    def _iter_records_from_json_bytes(self, raw_bytes: bytes) -> List[Dict]:
        """JSON bytesë¥¼ dict ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        text = None
        for enc in ("utf-8", "utf-8-sig", "cp949", "euc-kr"):
            try:
                text = raw_bytes.decode(enc)
                break
            except Exception:
                continue
        if text is None:
            return []
        try:
            obj = json.loads(text)
        except Exception:
            return []
        return self._flatten_json_records(obj)

    def _iter_json_records_in_path(self, path: Path) -> Iterable[Dict]:
        """ê²½ë¡œ(íŒŒì¼/í´ë”) ë‚´ë¶€ì˜ json/jsonl/zip ë ˆì½”ë“œë¥¼ ìˆœíšŒ"""
        if not path.exists():
            return

        if path.is_file():
            files = [path]
        else:
            files = sorted(p for p in path.rglob("*") if p.is_file())

        for file_path in files:
            suffix = file_path.suffix.lower()
            try:
                if suffix == ".jsonl":
                    with open(file_path, "r", encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if not line:
                                continue
                            try:
                                obj = json.loads(line)
                            except Exception:
                                continue
                            if isinstance(obj, dict):
                                yield obj
                elif suffix == ".json":
                    with open(file_path, "rb") as f:
                        for rec in self._iter_records_from_json_bytes(f.read()):
                            yield rec
                elif suffix == ".zip":
                    with zipfile.ZipFile(file_path) as zf:
                        members = [m for m in zf.namelist() if m.lower().endswith(".json")]
                        for member in members:
                            try:
                                raw = zf.read(member)
                            except Exception:
                                continue
                            for rec in self._iter_records_from_json_bytes(raw):
                                yield rec
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")

    def load_classics_data(self) -> List[Dict]:
        """
        ê³ ì „ë¬¸í•™ 600ê°œ ë°ì´í„° ë¡œë“œ

        TODO: ì‹¤ì œ AI HUB ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”

        Returns:
            List[Dict]: ê³ ì „ë¬¸í•™ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            ì˜ˆìƒ êµ¬ì¡°:
            {
                "id": "classic_001",
                "source": "ì¶˜í–¥ì „",
                "passage": "ì§€ë¬¸ ë‚´ìš©...",
                "question": "ì§ˆë¬¸...",
                "answer": "ëª¨ë²” ë‹µì•ˆ..."
            }
        """
        paths = self._normalize_paths(self.raw_classics_path)
        if not paths:
            print("âš ï¸ ê³ ì „ë¬¸í•™ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   configs/training_config.yamlì˜ data.raw_classics_pathë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return []

        loaded_data: List[Dict] = []
        for path in paths:
            if not path.exists():
                print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
                continue
            for raw_item in tqdm(self._iter_json_records_in_path(path), desc=f"ê³ ì „ë¬¸í•™ ë¡œë”© ({path.name})"):
                parsed = self.parse_classic_text(raw_item)
                if parsed.get("passage"):
                    parsed["dataset"] = "classics"
                    loaded_data.append(parsed)

        print(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {', '.join(str(p) for p in paths)}")
        print(f"âœ… ê³ ì „ë¬¸í•™ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(loaded_data)}ê°œ")
        return loaded_data

    def _flatten_json_records(self, obj: Any) -> List[Dict]:
        """JSON ê°ì²´ë¥¼ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™”"""
        if isinstance(obj, list):
            return [x for x in obj if isinstance(x, dict)]

        if isinstance(obj, dict):
            candidate_keys = ["data", "items", "records", "dataset", "documents", "annotations"]
            for key in candidate_keys:
                value = obj.get(key)
                if isinstance(value, list):
                    return [x for x in value if isinstance(x, dict)]
            return [obj]

        return []

    def load_comprehension_data(self) -> List[Dict]:
        """
        êµ­ì–´ êµê³¼ ì§€ë¬¸í˜• ë¬¸ì œ ë°ì´í„° ë¡œë“œ (1.26GB)

        TODO: ì‹¤ì œ AI HUB ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”

        Returns:
            List[Dict]: ì§€ë¬¸í˜• ë¬¸ì œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        paths = self._normalize_paths(self.raw_comprehension_path)
        if not paths:
            print("âš ï¸ ì§€ë¬¸í˜• ë¬¸ì œ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        loaded_data: List[Dict] = []
        for path in paths:
            if not path.exists():
                print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
                continue

            for raw_item in tqdm(self._iter_json_records_in_path(path), desc=f"ì§€ë¬¸í˜• ë¡œë”© ({path.name})"):
                parsed = self.parse_comprehension_item(raw_item)
                if parsed:
                    loaded_data.append(parsed)

        print(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {', '.join(str(p) for p in paths)}")
        print(f"âœ… ì§€ë¬¸í˜• ë¬¸ì œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(loaded_data)}ê°œ")
        return loaded_data

    def load_evaluation_data(self) -> List[Dict]:
        """
        ë…¼ìˆ í˜•/ì„œìˆ í˜• í‰ê°€ ë°ì´í„° ë¡œë“œ (232MB)

        TODO: ì‹¤ì œ AI HUB ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”

        Returns:
            List[Dict]: í‰ê°€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        paths = self._normalize_paths(self.raw_evaluation_path)
        if not paths:
            print("âš ï¸ í‰ê°€ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        loaded_data: List[Dict] = []
        for path in paths:
            if not path.exists():
                print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
                continue

            for raw_item in tqdm(self._iter_json_records_in_path(path), desc=f"í‰ê°€ë°ì´í„° ë¡œë”© ({path.name})"):
                parsed = self.parse_evaluation_item(raw_item)
                if parsed:
                    loaded_data.append(parsed)

        print(f"ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {', '.join(str(p) for p in paths)}")
        print(f"âœ… í‰ê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(loaded_data)}ê°œ")
        return loaded_data

    def parse_classic_text(self, raw_data: Dict) -> Dict:
        """
        ê³ ì „ë¬¸í•™ ì›ë³¸ ë°ì´í„° íŒŒì‹±

        TODO: AI HUB ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”

        Args:
            raw_data: ì›ë³¸ ë°ì´í„°

        Returns:
            Dict: íŒŒì‹±ëœ ë°ì´í„°
            {
                "id": str,
                "source": str,  # ì‘í’ˆëª…
                "passage": str,  # ì§€ë¬¸
                "question": str,  # ì§ˆë¬¸
                "answer": str  # ëª¨ë²” ë‹µì•ˆ
            }
        """
        metadata = raw_data.get("metadata", {}) if isinstance(raw_data.get("metadata"), dict) else {}

        def pick_first(keys: List[str], default: str = "") -> str:
            for key in keys:
                value = raw_data.get(key)
                if value is None:
                    continue
                if isinstance(value, list):
                    value = " ".join(str(v) for v in value if v is not None)
                value = str(value).strip()
                if value:
                    return value
            return default

        source = (
            pick_first(["source", "ì‘í’ˆëª…", "work_title", "title"])
            or str(metadata.get("title", "")).strip()
            or "ê³ ì „ë¬¸í•™"
        )
        passage = (
            pick_first(["passage", "ì§€ë¬¸", "text", "content", "ë³¸ë¬¸"])
            or str(raw_data.get("ë¬¸ì œì§€ë¬¸", "")).strip()
        )
        question = pick_first(["question", "ì§ˆë¬¸", "ë¬¸í•­", "query", "prompt"])
        answer = pick_first(["answer", "ì •ë‹µ", "í•´ì„¤", "ëª¨ë²”ë‹µì•ˆ", "explanation"])

        if not question:
            question = f"{source}ì˜ í•µì‹¬ ì£¼ì œì™€ í™”ìì˜ íƒœë„ë¥¼ ì„¤ëª…í•´ ë³´ì„¸ìš”."
        if not answer:
            answer = (
                "ì‘í’ˆì˜ í•µì‹¬ ì†Œì¬ì™€ í‘œí˜„ ë°©ì‹ì„ ê·¼ê±°ë¡œ ì£¼ì œ ì˜ì‹ì„ ì„¤ëª…í•˜ê³ , "
                "í™”ìì˜ ì •ì„œ ë³€í™”ì™€ ì‹œëŒ€ì  ë§¥ë½ì„ í•¨ê»˜ í•´ì„í•´ ë³´ì„¸ìš”."
            )

        data_id = (
            pick_first(["id", "data_id"])
            or str(metadata.get("data_id", "")).strip()
            or f"item_{abs(hash((source, passage[:120]))) % 10**10}"
        )

        return {
            "id": data_id,
            "source": source,
            "passage": passage,
            "question": question,
            "answer": answer
        }

    def parse_comprehension_item(self, raw_data: Dict) -> Dict:
        """êµ­ì–´ êµê³¼ ì§€ë¬¸í˜• ë¬¸ì œ ë°ì´í„° íŒŒì‹±"""
        if not isinstance(raw_data, dict):
            return {}
        if "learning_data_info" not in raw_data:
            return {}

        class_map: Dict[str, List[str]] = {}
        for block in raw_data.get("learning_data_info", []):
            class_name = str(block.get("class_name", "")).strip()
            if not class_name:
                continue
            texts: List[str] = []
            for info in block.get("class_info_list", []):
                txt = str(info.get("text_description", "")).strip()
                if txt:
                    texts.append(txt)
            if texts:
                class_map.setdefault(class_name, []).extend(texts)

        passage_texts = class_map.get("ì§€ë¬¸", []) + class_map.get("ì§€ë¬¸(ì´ë¯¸ì§€)", [])
        question_texts = class_map.get("ë¬¸í•­", [])
        answer_texts = class_map.get("ì •ë‹µ", []) + class_map.get("í•´ì„¤", [])

        passage = "\n".join(passage_texts).strip()
        question = "\n".join(question_texts).strip()
        answer = "\n".join(answer_texts).strip()

        if not (passage and question and answer):
            return {}

        raw_info = raw_data.get("raw_data_info", {})
        src_info = raw_data.get("source_data_info", {})
        source_name = str(src_info.get("source_data_name", "")).strip()
        school = str(raw_info.get("school", "")).strip()
        grade = str(raw_info.get("grade", "")).strip()
        subject = str(raw_info.get("subject", "")).strip()
        source = " / ".join(x for x in [school, grade, subject, source_name] if x)

        return {
            "id": source_name or f"comp_{abs(hash((question[:80], passage[:80]))) % 10**10}",
            "source": source or "êµ­ì–´ êµê³¼ ì§€ë¬¸í˜• ë¬¸ì œ",
            "passage": passage,
            "question": question,
            "answer": answer,
            "dataset": "comprehension"
        }

    def parse_evaluation_item(self, raw_data: Dict) -> Dict:
        """ë…¼ìˆ í˜•/ì„œìˆ í˜• í‰ê°€ ë°ì´í„° íŒŒì‹±"""
        if not isinstance(raw_data, dict):
            return {}
        if "essay_question" not in raw_data or "essay_answer" not in raw_data:
            return {}

        question_obj = raw_data.get("essay_question", {})
        answer_obj = raw_data.get("essay_answer", {})
        rubric_obj = raw_data.get("rubric", {})

        prompt = str(question_obj.get("prompt", "")).strip()
        answer_text = str(answer_obj.get("text", "")).strip()
        topic = str(question_obj.get("topic", "")).strip()
        qtype = str(question_obj.get("type", "")).strip()
        grade = str(question_obj.get("grade", "")).strip()
        subject = str(question_obj.get("subject", "")).strip()
        keyword = question_obj.get("keyword", [])
        if isinstance(keyword, list):
            keyword_text = ", ".join(str(k).strip() for k in keyword if str(k).strip())
        else:
            keyword_text = str(keyword).strip()

        if not (prompt and answer_text):
            return {}

        passage_parts = [f"ì£¼ì œ: {topic}" if topic else "", prompt]
        if keyword_text:
            passage_parts.append(f"í•µì‹¬ì–´: {keyword_text}")
        achievement = str(rubric_obj.get("achievement", "")).strip()
        if achievement:
            passage_parts.append(f"í‰ê°€ê¸°ì¤€: {achievement}")
        passage = "\n".join(x for x in passage_parts if x).strip()

        qid = str(question_obj.get("id", "")).strip()
        aid = str(answer_obj.get("id", "")).strip()
        source = " / ".join(x for x in [qtype, grade, subject] if x)

        return {
            "id": f"{qid}_{aid}" if (qid or aid) else f"eval_{abs(hash((prompt[:80], answer_text[:80]))) % 10**10}",
            "source": source or "ë…¼ìˆ /ì„œìˆ í˜• í‰ê°€",
            "passage": passage,
            "question": prompt,
            "answer": answer_text,
            "dataset": "evaluation"
        }

    def filter_quality_data(
        self,
        data: List[Dict],
        min_passage_length: int = 50,
        min_answer_length: int = 20
    ) -> List[Dict]:
        """
        ì €í’ˆì§ˆ ë°ì´í„° í•„í„°ë§

        Args:
            data: ì›ë³¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            min_passage_length: ìµœì†Œ ì§€ë¬¸ ê¸¸ì´
            min_answer_length: ìµœì†Œ ë‹µë³€ ê¸¸ì´

        Returns:
            List[Dict]: í•„í„°ë§ëœ ë°ì´í„°
        """
        filtered = []

        for item in data:
            passage = item.get("passage", "")
            answer = item.get("answer", "")
            question = item.get("question", "")

            # ê¸°ë³¸ í’ˆì§ˆ ê²€ì¦
            if len(passage) < min_passage_length:
                continue
            if len(answer) < min_answer_length:
                continue
            if not question.strip():
                continue

            filtered.append(item)

        if data:
            ratio = len(filtered) / len(data) * 100
            print(f"í’ˆì§ˆ í•„í„°ë§: {len(data)} â†’ {len(filtered)} ({ratio:.1f}%)")
        else:
            print("í’ˆì§ˆ í•„í„°ë§: ì…ë ¥ ë°ì´í„° 0ê°œ")
        return filtered

    def split_train_valid(
        self,
        data: List[Dict],
        train_ratio: float = 0.8,
        seed: int = 42
    ) -> tuple:
        """
        í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 

        Args:
            data: ì „ì²´ ë°ì´í„°
            train_ratio: í•™ìŠµ ë°ì´í„° ë¹„ìœ¨
            seed: ëœë¤ ì‹œë“œ

        Returns:
            tuple: (train_data, valid_data)
        """
        import random
        random.seed(seed)

        shuffled = data.copy()
        random.shuffle(shuffled)

        split_idx = int(len(shuffled) * train_ratio)
        train_data = shuffled[:split_idx]
        valid_data = shuffled[split_idx:]

        print(f"ë°ì´í„° ë¶„í• : Train {len(train_data)}ê°œ, Valid {len(valid_data)}ê°œ")
        return train_data, valid_data

    def save_jsonl(self, data: List[Dict], output_path: str):
        """JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path} ({len(data)}ê°œ)")

    def load_jsonl(self, input_path: str) -> List[Dict]:
        """JSONL í˜•ì‹ ë¡œë“œ"""
        data = []
        with open(input_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        return data

    def preprocess_pipeline(self) -> tuple:
        """
        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Returns:
            tuple: (train_data, valid_data)
        """
        print("=" * 60)
        print("ğŸ“š ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)

        # 1. ë°ì´í„° ë¡œë“œ
        classics_data = self.load_classics_data()
        comprehension_data = self.load_comprehension_data()
        evaluation_data = self.load_evaluation_data()

        merged = classics_data + comprehension_data + evaluation_data

        if not merged:
            print("\nâš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:")
            print("1. AI HUBì—ì„œ ê³ ì „ë¬¸í•™ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
            print("2. configs/training_config.yamlì— ë°ì´í„° ê²½ë¡œ ì„¤ì •")
            print("3. ë°ì´í„° ê²½ë¡œì™€ ì••ì¶• íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸")
            return [], []

        # 2. ë°ì´í„° íŒŒì‹± (ì´ë¯¸ ì •ê·œí™”ëœ í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        parsed_data = []
        for item in merged:
            if {"id", "source", "passage", "question", "answer"}.issubset(item.keys()):
                parsed_data.append(item)
            else:
                parsed_data.append(self.parse_classic_text(item))

        print(
            f"ğŸ“Š ë¡œë“œ í†µê³„ - ê³ ì „ë¬¸í•™: {len(classics_data)}, "
            f"ì§€ë¬¸í˜•: {len(comprehension_data)}, í‰ê°€: {len(evaluation_data)}, "
            f"ì´í•©: {len(parsed_data)}"
        )

        # 3. í’ˆì§ˆ í•„í„°ë§
        filtered_data = self.filter_quality_data(parsed_data)

        # 4. í•™ìŠµ/ê²€ì¦ ë¶„í• 
        train_data, valid_data = self.split_train_valid(filtered_data)

        # 5. ì €ì¥
        self.save_jsonl(train_data, self.output_dir / "train_raw.jsonl")
        self.save_jsonl(valid_data, self.output_dir / "valid_raw.jsonl")

        print("=" * 60)
        print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 60)

        return train_data, valid_data


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    preprocessor = DataPreprocessor()
    train_data, valid_data = preprocessor.preprocess_pipeline()
