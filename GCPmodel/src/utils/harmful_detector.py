"""
ìœ í•´í‘œí˜„ ê°ì§€ ëª¨ë“ˆ
AI HUB ìœ í•´í‘œí˜„ ê²€ì¶œ AI ëª¨ë¸ ì—°ë™
"""

import os
from pathlib import Path
from typing import Dict, List, Optional


class HarmfulExpressionDetector:
    """AI HUB ìœ í•´í‘œí˜„ ê²€ì¶œ AI ëª¨ë¸ ë˜í¼"""

    def __init__(
        self,
        model_path: str = "",  # TODO: AI HUB ìœ í•´í‘œí˜„ ëª¨ë¸ ê²½ë¡œ
        device: str = "auto"
    ):
        """
        Args:
            model_path: AI HUB ìœ í•´í‘œí˜„ ëª¨ë¸ ê²½ë¡œ
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("auto", "cuda", "cpu")
        """
        self.model_path = model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        self.is_loaded = False

        # ìœ í•´í‘œí˜„ ì¹´í…Œê³ ë¦¬
        self.categories = {
            "ìš•ì„¤": "swear",
            "ë¹„ì†ì–´": "profanity",
            "í˜ì˜¤í‘œí˜„": "hate_speech",
            "ì„±ì í‘œí˜„": "sexual",
            "í­ë ¥í‘œí˜„": "violence",
            "ì°¨ë³„í‘œí˜„": "discrimination"
        }

        # ê²½ê³  ë ˆë²¨ ì„ê³„ê°’
        self.thresholds = {
            "safe": 0,
            "caution": 2,
            "warning": 3
        }

        if model_path and Path(model_path).exists():
            self.load_model()

    def load_model(self):
        """
        AI HUB ìœ í•´í‘œí˜„ ëª¨ë¸ ë¡œë“œ

        TODO: ì‹¤ì œ AI HUB ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„ í•„ìš”
        AI HUB ëª¨ë¸ íŒŒì¼ êµ¬ì¡°:
        - model.pt ë˜ëŠ” model.bin (ëª¨ë¸ ê°€ì¤‘ì¹˜)
        - config.json (ëª¨ë¸ ì„¤ì •)
        - vocab.txt ë˜ëŠ” tokenizer.json (í† í¬ë‚˜ì´ì €)
        """
        if not self.model_path:
            print("âš ï¸ ìœ í•´í‘œí˜„ ëª¨ë¸ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        model_dir = Path(self.model_path)
        if not model_dir.exists():
            print(f"âš ï¸ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            return

        print(f"ğŸ“¦ ìœ í•´í‘œí˜„ ëª¨ë¸ ë¡œë”©: {self.model_path}")

        # TODO: AI HUB ëª¨ë¸ ë¡œë”© ë¡œì§ êµ¬í˜„
        # ì˜ˆì‹œ êµ¬ì¡° (ì‹¤ì œ ëª¨ë¸ì— ë§ê²Œ ìˆ˜ì • í•„ìš”):
        """
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification

        # ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = self.model.to(self.device)
        self.model.eval()

        self.is_loaded = True
        print("âœ… ìœ í•´í‘œí˜„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        """

        print("   âš ï¸ ëª¨ë¸ ë¡œë”© ë¡œì§ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.")
        print("   AI HUB ëª¨ë¸ êµ¬ì¡°ì— ë§ê²Œ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    def detect(self, text: str) -> Dict:
        """
        ìœ í•´í‘œí˜„ ê°ì§€

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸

        Returns:
            Dict: {
                "is_harmful": bool,
                "harmful_expressions": List[Dict],
                "warning_level": str,
                "scores": Dict[str, float]
            }
        """
        if not text.strip():
            return self._empty_result()

        if not self.is_loaded:
            # ëª¨ë¸ ì—†ì´ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì§€ (fallback)
            return self._keyword_based_detection(text)

        # TODO: ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  êµ¬í˜„
        """
        import torch

        # í† í°í™”
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)

        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)

        # ê²°ê³¼ íŒŒì‹±
        scores = {cat: float(probs[0][i]) for i, cat in enumerate(self.categories.values())}
        harmful_expressions = [
            {"category": cat_kr, "score": scores[cat_en]}
            for cat_kr, cat_en in self.categories.items()
            if scores[cat_en] > 0.5
        ]

        return {
            "is_harmful": len(harmful_expressions) > 0,
            "harmful_expressions": harmful_expressions,
            "warning_level": self._get_warning_level(len(harmful_expressions)),
            "scores": scores
        }
        """

        return self._keyword_based_detection(text)

    def _keyword_based_detection(self, text: str) -> Dict:
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ìœ í•´í‘œí˜„ ê°ì§€ (ëª¨ë¸ ì—†ì„ ë•Œ fallback)

        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸

        Returns:
            Dict: ê°ì§€ ê²°ê³¼
        """
        # ê¸°ë³¸ ìœ í•´í‘œí˜„ í‚¤ì›Œë“œ (ì˜ˆì‹œ)
        # ì‹¤ì œ ì‚¬ìš© ì‹œ ë” í¬ê´„ì ì¸ ëª©ë¡ í•„ìš”
        harmful_keywords = {
            "ìš•ì„¤": ["ë°”ë³´", "ë©ì²­", "ì§œì¦"],
            "ë¹„ì†ì–´": [],
            "í˜ì˜¤í‘œí˜„": [],
            "ì°¨ë³„í‘œí˜„": []
        }

        detected = []
        for category, keywords in harmful_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    detected.append({
                        "category": category,
                        "keyword": keyword,
                        "score": 1.0
                    })

        warning_level = self._get_warning_level(len(detected))

        return {
            "is_harmful": len(detected) > 0,
            "harmful_expressions": detected,
            "warning_level": warning_level,
            "detection_method": "keyword_based",
            "note": "AI ëª¨ë¸ ë¡œë“œ í›„ ë” ì •í™•í•œ ê°ì§€ ê°€ëŠ¥"
        }

    def _get_warning_level(self, count: int) -> str:
        """ê²½ê³  ë ˆë²¨ ê²°ì •"""
        if count == 0:
            return "ì•ˆì „"
        elif count <= self.thresholds["caution"]:
            return "ì£¼ì˜"
        else:
            return "ê²½ê³ "

    def _empty_result(self) -> Dict:
        """ë¹ˆ ê²°ê³¼"""
        return {
            "is_harmful": False,
            "harmful_expressions": [],
            "warning_level": "ì•ˆì „",
            "scores": {}
        }

    def batch_detect(self, texts: List[str]) -> List[Dict]:
        """
        ë°°ì¹˜ ê°ì§€

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[Dict]: ê°ì§€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        return [self.detect(text) for text in texts]

    def get_report(self, detection_result: Dict) -> str:
        """
        ê°ì§€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            detection_result: detect() ê²°ê³¼

        Returns:
            str: ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
        """
        report = "## ìœ í•´í‘œí˜„ ê°ì§€ ê²°ê³¼\n\n"
        report += f"- **ê²½ê³  ë ˆë²¨**: {detection_result.get('warning_level', 'N/A')}\n"
        report += f"- **ìœ í•´í‘œí˜„ ì—¬ë¶€**: {'ì˜ˆ' if detection_result.get('is_harmful') else 'ì•„ë‹ˆì˜¤'}\n\n"

        expressions = detection_result.get('harmful_expressions', [])
        if expressions:
            report += "### ê°ì§€ëœ í‘œí˜„\n"
            for expr in expressions:
                report += f"- **{expr.get('category', 'N/A')}**: "
                if 'keyword' in expr:
                    report += f"'{expr['keyword']}'"
                if 'score' in expr:
                    report += f" (ì‹ ë¢°ë„: {expr['score']:.2f})"
                report += "\n"
        else:
            report += "ê°ì§€ëœ ìœ í•´í‘œí˜„ì´ ì—†ìŠµë‹ˆë‹¤.\n"

        return report


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    detector = HarmfulExpressionDetector(
        model_path=""  # TODO: AI HUB ëª¨ë¸ ê²½ë¡œ
    )

    test_text = "ì´ ë¶€ë¶„ì´ ì´í•´ê°€ ì˜ ì•ˆ ë©ë‹ˆë‹¤. ì¢€ ë” ì„¤ëª…í•´ì£¼ì„¸ìš”."
    result = detector.detect(test_text)

    print("=" * 60)
    print("ğŸ” ìœ í•´í‘œí˜„ ê°ì§€ ê²°ê³¼")
    print("=" * 60)

    import json
    print(json.dumps(result, ensure_ascii=False, indent=2))

    print("\n" + detector.get_report(result))
