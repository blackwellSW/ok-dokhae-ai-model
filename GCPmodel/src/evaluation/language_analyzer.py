"""
ì–¸ì–´ ë¶„ì„ ëª¨ë“ˆ (ì •ëŸ‰ í‰ê°€)
- ì–´íœ˜ ë‹¤ì–‘ì„± (MTLD ê¸°ë°˜)
- í•µì‹¬ ê°œë…ì–´ ì‚¬ìš© (ì„ë² ë”© ê¸°ë°˜)
- ë¬¸ì¥ ë³µì¡ë„
- ë°˜ë³µ íŒ¨í„´
- ê°ì • í†¤ (ë§¥ë½ ê³ ë ¤ AI)
- ìœ í•´í‘œí˜„ ê°ì§€ (AI HUB ëª¨ë¸)
"""

import re
import math
from collections import Counter
from typing import Dict, List, Tuple, Optional

import numpy as np


class ComprehensiveLanguageAnalyzer:
    """í†µí•© ì–¸ì–´ ë¶„ì„ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        harmful_model_path: str = "",  # TODO: ìœ í•´í‘œí˜„ AI ëª¨ë¸ ê²½ë¡œ
        use_gpu: bool = True
    ):
        """
        Args:
            harmful_model_path: AI HUB ìœ í•´í‘œí˜„ ëª¨ë¸ ê²½ë¡œ
            use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        """
        self.harmful_model_path = harmful_model_path
        self.use_gpu = use_gpu

        # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.okt = None
        self._init_morpheme_analyzer()

        # ê°œì„ ëœ ë¶„ì„ê¸°ë“¤
        self.vocab_analyzer = ImprovedVocabularyAnalyzer()
        self.concept_analyzer = SemanticConceptAnalyzer(use_gpu=use_gpu)
        self.sentiment_analyzer = AdvancedSentimentAnalyzer(use_gpu=use_gpu)

        # ìœ í•´í‘œí˜„ AI ëª¨ë¸
        self.harmful_detector = None
        if harmful_model_path:
            self._load_harmful_model(harmful_model_path)

    def _init_morpheme_analyzer(self):
        """í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        try:
            from konlpy.tag import Okt
            self.okt = Okt()
        except ImportError:
            print("âš ï¸ konlpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install konlpy")
            self.okt = None

    def _load_harmful_model(self, path: str):
        """AI HUB ìœ í•´í‘œí˜„ AI ëª¨ë¸ ë¡œë“œ"""
        # TODO: ì‹¤ì œ AI HUB ëª¨ë¸ ë¡œë”© êµ¬í˜„
        # ëª¨ë¸ êµ¬ì¡°ì— ë”°ë¼ ë¡œë”© ì½”ë“œ ì‘ì„± í•„ìš”
        print(f"âš ï¸ ìœ í•´í‘œí˜„ ëª¨ë¸ ë¡œë”©: {path}")
        print("   ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì½”ë“œë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”.")
        self.harmful_detector = None

    def analyze(self, student_text: str) -> Dict:
        """
        ì¢…í•© ë¶„ì„

        Args:
            student_text: í•™ìƒ í…ìŠ¤íŠ¸

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼
        """
        if not student_text.strip():
            return self._empty_result()

        # ê¸°ë³¸ í† í°í™”
        morphs = self._tokenize(student_text)
        sentences = self._split_sentences(student_text)

        return {
            # ê°œì„ ëœ ë¶„ì„
            "ì–´íœ˜_ë‹¤ì–‘ì„±": self.vocab_analyzer.calculate_diversity(student_text),
            "í•µì‹¬_ê°œë…ì–´": self.concept_analyzer.analyze_concepts(student_text),
            "ê°ì •_í†¤": self.sentiment_analyzer.analyze_sentiment(student_text),

            # ê¸°ì¡´ ë¶„ì„
            "ë¬¸ì¥_ë³µì¡ë„": self._calc_complexity(sentences, morphs),
            "ë°˜ë³µ_íŒ¨í„´": self._analyze_repetition(morphs),
            "ìœ í•´í‘œí˜„": self._detect_harmful(student_text),

            # í†µê³„
            "í†µê³„": {
                "ì´_ë‹¨ì–´": len(morphs),
                "ê³ ìœ _ë‹¨ì–´": len(set(morphs)),
                "ë¬¸ì¥_ìˆ˜": len(sentences),
                "í‰ê· _ë¬¸ì¥_ê¸¸ì´": round(len(morphs) / len(sentences), 1) if sentences else 0
            }
        }

    def _tokenize(self, text: str) -> List[str]:
        """í˜•íƒœì†Œ í† í°í™”"""
        if self.okt:
            return self.okt.morphs(text)
        # fallback: ê³µë°± ê¸°ì¤€ í† í°í™”
        return text.split()

    def _split_sentences(self, text: str) -> List[str]:
        """ë¬¸ì¥ ë¶„ë¦¬"""
        return [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]

    def _calc_complexity(self, sentences: List[str], morphs: List[str]) -> Dict:
        """ë¬¸ì¥ ë³µì¡ë„ ê³„ì‚°"""
        if not sentences:
            return {"ì ìˆ˜": 0, "ë“±ê¸‰": "N/A"}

        avg_length = len(morphs) / len(sentences)
        avg_clauses = sum(s.count(',') + 1 for s in sentences) / len(sentences)
        score = (avg_length / 10) + (avg_clauses * 2)

        if score >= 10:
            grade = "ë³µì¡í•¨"
        elif score >= 6:
            grade = "ì ì ˆí•¨"
        else:
            grade = "ë‹¨ìˆœí•¨"

        return {
            "ì ìˆ˜": round(score, 2),
            "ë“±ê¸‰": grade,
            "í‰ê· _ë¬¸ì¥_ê¸¸ì´": round(avg_length, 1),
            "í‰ê· _ì ˆ_ê°œìˆ˜": round(avg_clauses, 1)
        }

    def _analyze_repetition(self, morphs: List[str]) -> Dict:
        """ë°˜ë³µ íŒ¨í„´ ë¶„ì„"""
        if not morphs:
            return {"ê³¼ë„í•œ_ë°˜ë³µ": {}, "ë°˜ë³µë¥ ": 0, "í‰ê°€": "N/A"}

        freq = Counter(morphs)
        excessive = {w: c for w, c in freq.items() if c >= 3 and len(w) > 1}
        repetition_rate = sum(excessive.values()) / len(morphs) if morphs else 0

        return {
            "ê³¼ë„í•œ_ë°˜ë³µ": excessive,
            "ë°˜ë³µë¥ ": round(repetition_rate, 3),
            "í‰ê°€": "ì£¼ì˜" if repetition_rate > 0.2 else "ì–‘í˜¸"
        }

    def _detect_harmful(self, text: str) -> Dict:
        """ìœ í•´í‘œí˜„ ê°ì§€"""
        if self.harmful_detector:
            # TODO: ì‹¤ì œ ëª¨ë¸ ì˜ˆì¸¡ êµ¬í˜„
            result = {"harmful_expressions": []}
            detected_count = len(result.get('harmful_expressions', []))
        else:
            detected_count = 0

        if detected_count == 0:
            level = "ì•ˆì „"
        elif detected_count <= 2:
            level = "ì£¼ì˜"
        else:
            level = "ê²½ê³ "

        return {
            "ê°ì§€_ê°œìˆ˜": detected_count,
            "ê²½ê³ _ë ˆë²¨": level,
            "ì¡°ì¹˜": "êµì‚¬ ì•Œë¦¼ í•„ìš”" if level == "ê²½ê³ " else "ì •ìƒ"
        }

    def _empty_result(self) -> Dict:
        """ë¹ˆ ê²°ê³¼"""
        return {
            "ì–´íœ˜_ë‹¤ì–‘ì„±": {"ì ìˆ˜": 0, "ë“±ê¸‰": "N/A"},
            "í•µì‹¬_ê°œë…ì–´": {"ì´_ê°œë…_ì‚¬ìš©": 0, "í‰ê°€": "N/A"},
            "ê°ì •_í†¤": {"ìµœì¢…_í†¤": "N/A", "ì ìˆ˜": 0},
            "ë¬¸ì¥_ë³µì¡ë„": {"ì ìˆ˜": 0, "ë“±ê¸‰": "N/A"},
            "ë°˜ë³µ_íŒ¨í„´": {"ë°˜ë³µë¥ ": 0, "í‰ê°€": "N/A"},
            "ìœ í•´í‘œí˜„": {"ê°ì§€_ê°œìˆ˜": 0, "ê²½ê³ _ë ˆë²¨": "ì•ˆì „"},
            "í†µê³„": {"ì´_ë‹¨ì–´": 0, "ê³ ìœ _ë‹¨ì–´": 0, "ë¬¸ì¥_ìˆ˜": 0}
        }


class ImprovedVocabularyAnalyzer:
    """MTLD ê¸°ë°˜ ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„"""

    def __init__(self):
        self.okt = None
        try:
            from konlpy.tag import Okt
            self.okt = Okt()
        except ImportError:
            pass

    def calculate_diversity(self, text: str) -> Dict:
        """ë‹¤ì¸µì  ì–´íœ˜ ë‹¤ì–‘ì„± ë¶„ì„"""
        if self.okt:
            morphs = self.okt.morphs(text)
        else:
            morphs = text.split()

        if not morphs:
            return {"ì ìˆ˜": 0, "ë“±ê¸‰": "N/A", "í•´ì„": "í…ìŠ¤íŠ¸ ì—†ìŒ"}

        # 1. ê¸°ë³¸ TTR
        basic_ttr = len(set(morphs)) / len(morphs)

        # 2. MTLD
        mtld_score = self._calculate_mtld(morphs)

        # 3. í’ˆì‚¬ ë‹¤ì–‘ì„±
        pos_diversity = self._pos_diversity(text) if self.okt else 0.5

        # 4. í•™ë¬¸ì  ì–´íœ˜
        academic_score = self._academic_vocabulary(morphs)

        # ìµœì¢… ì ìˆ˜
        final_score = (
            basic_ttr * 0.3 +
            mtld_score * 0.4 +
            pos_diversity * 0.2 +
            academic_score * 0.1
        )

        return {
            "ì ìˆ˜": round(final_score, 3),
            "ë“±ê¸‰": self._grade(final_score),
            "ì„¸ë¶€": {
                "ê¸°ë³¸_TTR": round(basic_ttr, 3),
                "MTLD": round(mtld_score, 3),
                "í’ˆì‚¬_ë‹¤ì–‘ì„±": round(pos_diversity, 3),
                "í•™ë¬¸ì _ì–´íœ˜": round(academic_score, 3)
            },
            "í•´ì„": self._interpret(final_score)
        }

    def _calculate_mtld(self, morphs: List[str], threshold: float = 0.72) -> float:
        """MTLD ê³„ì‚° (í…ìŠ¤íŠ¸ ê¸¸ì´ ë³´ì •)"""
        if len(morphs) < 10:
            return len(set(morphs)) / len(morphs) if morphs else 0

        factors = []
        start = 0

        for i in range(10, len(morphs)):
            segment = morphs[start:i]
            ttr = len(set(segment)) / len(segment)
            if ttr < threshold:
                factors.append(i - start)
                start = i

        if start < len(morphs):
            factors.append(len(morphs) - start)

        avg_factor = sum(factors) / len(factors) if factors else 10
        return min(avg_factor / 50, 1.0)

    def _pos_diversity(self, text: str) -> float:
        """í’ˆì‚¬ ë‹¤ì–‘ì„± (ì—”íŠ¸ë¡œí”¼)"""
        if not self.okt:
            return 0.5

        pos = self.okt.pos(text)
        pos_counts = {"Noun": 0, "Verb": 0, "Adjective": 0, "Adverb": 0}

        for word, tag in pos:
            if tag.startswith("N"):
                pos_counts["Noun"] += 1
            elif tag.startswith("V"):
                pos_counts["Verb"] += 1
            elif tag.startswith("Adj"):
                pos_counts["Adjective"] += 1
            elif tag.startswith("Adv"):
                pos_counts["Adverb"] += 1

        total = sum(pos_counts.values())
        if total == 0:
            return 0

        entropy = 0
        for count in pos_counts.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)

        return entropy / 2  # ì •ê·œí™”

    def _academic_vocabulary(self, morphs: List[str]) -> float:
        """í•™ë¬¸ì  ì–´íœ˜ ë¹„ìœ¨"""
        academic_count = sum(1 for m in morphs if len(m) >= 3)
        return academic_count / len(morphs) if morphs else 0

    def _grade(self, score: float) -> str:
        if score >= 0.75:
            return "ìš°ìˆ˜"
        elif score >= 0.6:
            return "ì–‘í˜¸"
        elif score >= 0.4:
            return "ë³´í†µ"
        else:
            return "ê°œì„ í•„ìš”"

    def _interpret(self, score: float) -> str:
        if score >= 0.75:
            return "í’ë¶€í•˜ê³  ì •êµí•œ ì–´íœ˜ ì‚¬ìš©. í•™ë¬¸ì  í‘œí˜„ë ¥ ìš°ìˆ˜."
        elif score >= 0.6:
            return "ì ì ˆí•œ ì–´íœ˜ ì‚¬ìš©. ë‹¤ì–‘ì„± ì–‘í˜¸."
        elif score >= 0.4:
            return "ê¸°ë³¸ì  ì–´íœ˜ ì‚¬ìš©. í‘œí˜„ë ¥ í–¥ìƒ í•„ìš”."
        else:
            return "ì œí•œì  ì–´íœ˜. ë‹¤ì–‘í•œ í‘œí˜„ ì—°ìŠµ ê¶Œì¥."


class SemanticConceptAnalyzer:
    """ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê°œë…ì–´ ë¶„ì„"""

    def __init__(self, use_gpu: bool = True):
        self.use_gpu = use_gpu
        self.model = None
        self.okt = None

        # í•µì‹¬ ê°œë… ì¹´í…Œê³ ë¦¬
        self.concept_categories = {
            "ë¬¸í•™ì _ê¸°ë²•": ["ìƒì§•", "ì€ìœ ", "ë³µì„ ", "ë°˜ì „", "ê°ˆë“±êµ¬ì¡°", "ì¸ë¬¼í˜•ìƒí™”"],
            "ì‚¬íšŒë¬¸í™”ì _ë§¥ë½": ["ì‹ ë¶„ì œ", "ì‹œëŒ€ì ë°°ê²½", "ê³„ê¸‰ê°ˆë“±", "ìœ êµì‚¬ìƒ", "ê°€ë¶€ì¥ì œ"],
            "ì¸ê°„ê´€ê³„_ì‹¬ë¦¬": ["ì‚¬ë‘", "íš¨", "ì¶©", "ì ˆê°œ", "ìš•ë§", "ê°ˆë“±", "í™”í•´", "í¬ìƒ"],
            "ì£¼ì œ_ë©”ì‹œì§€": ["ì£¼ì œ", "êµí›ˆ", "í’ì", "ë¹„íŒ", "ê°€ì¹˜ê´€", "ì´ìƒ"]
        }

        self.category_embeddings = {}
        self._init_models()

    def _init_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from konlpy.tag import Okt
            self.okt = Okt()
        except ImportError:
            pass

        try:
            from sentence_transformers import SentenceTransformer
            device = "cuda" if self.use_gpu else "cpu"
            self.model = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)

            # ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ìƒì„±
            for cat, words in self.concept_categories.items():
                self.category_embeddings[cat] = self.model.encode(words)
        except ImportError:
            print("âš ï¸ sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.model = None

    def analyze_concepts(self, student_text: str) -> Dict:
        """ì˜ë¯¸ ê¸°ë°˜ ê°œë…ì–´ ë¶„ì„"""
        # ëª…ì‚¬ ì¶”ì¶œ
        if self.okt:
            nouns = self.okt.nouns(student_text)
        else:
            nouns = student_text.split()

        if not nouns:
            return self._empty_result()

        if not self.model:
            # ëª¨ë¸ ì—†ìœ¼ë©´ ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­
            return self._simple_matching(nouns)

        # í›„ë³´ ì„ë² ë”©
        candidates = list(set(nouns))
        candidate_embeddings = self.model.encode(candidates)

        # ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¹­
        category_matches = {}
        all_similarities = []
        threshold = 0.6

        for cat_name, cat_emb in self.category_embeddings.items():
            similarities = np.dot(candidate_embeddings, cat_emb.T)
            matches = []

            for i, cand in enumerate(candidates):
                max_sim = similarities[i].max()
                if max_sim >= threshold:
                    matched = self.concept_categories[cat_name][similarities[i].argmax()]
                    matches.append({
                        "í•™ìƒí‘œí˜„": cand,
                        "ë§¤ì¹­ê°œë…": matched,
                        "ìœ ì‚¬ë„": round(float(max_sim), 3)
                    })
                    all_similarities.append(max_sim)

            if matches:
                category_matches[cat_name] = matches

        total_matches = sum(len(m) for m in category_matches.values())
        avg_similarity = float(np.mean(all_similarities)) if all_similarities else 0
        coverage = len(category_matches)

        return {
            "ì¹´í…Œê³ ë¦¬ë³„_ë§¤ì¹­": category_matches,
            "ì´_ê°œë…_ì‚¬ìš©": total_matches,
            "í‰ê· _ìœ ì‚¬ë„": round(avg_similarity, 3),
            "ì»¤ë²„ë¦¬ì§€": coverage,
            "í‰ê°€": self._evaluate(total_matches, coverage),
            "í•´ì„": self._interpret(total_matches, coverage)
        }

    def _simple_matching(self, nouns: List[str]) -> Dict:
        """ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ (ëª¨ë¸ ì—†ì„ ë•Œ)"""
        all_concepts = []
        for words in self.concept_categories.values():
            all_concepts.extend(words)

        matches = [n for n in nouns if n in all_concepts]

        return {
            "ì¹´í…Œê³ ë¦¬ë³„_ë§¤ì¹­": {},
            "ì´_ê°œë…_ì‚¬ìš©": len(matches),
            "í‰ê· _ìœ ì‚¬ë„": 1.0 if matches else 0,
            "ì»¤ë²„ë¦¬ì§€": 0,
            "í‰ê°€": "ì–‘í˜¸" if len(matches) >= 3 else "ë³´í†µ" if matches else "ë¶€ì¡±",
            "í•´ì„": "ë‹¨ìˆœ ë§¤ì¹­ ê²°ê³¼ (ì„ë² ë”© ëª¨ë¸ ì—†ìŒ)"
        }

    def _evaluate(self, total: int, coverage: int) -> str:
        if total >= 5 and coverage >= 3:
            return "ìš°ìˆ˜"
        elif total >= 3 and coverage >= 2:
            return "ì–‘í˜¸"
        elif total >= 1:
            return "ë³´í†µ"
        else:
            return "ë¶€ì¡±"

    def _interpret(self, total: int, coverage: int) -> str:
        if total >= 5 and coverage >= 3:
            return "ë‹¤ì–‘í•œ ë¬¸í•™ì  ê°œë…ì„ ì ì ˆíˆ í™œìš©"
        elif total >= 3:
            return "í•µì‹¬ ê°œë…ì„ ë¶€ë¶„ì ìœ¼ë¡œ ì‚¬ìš©"
        else:
            return "ê°œë…ì  ìš©ì–´ ì‚¬ìš© ë¶€ì¡±. ë¬¸í•™ì  ê°œë… í™œìš© ê¶Œì¥"

    def _empty_result(self) -> Dict:
        return {
            "ì¹´í…Œê³ ë¦¬ë³„_ë§¤ì¹­": {},
            "ì´_ê°œë…_ì‚¬ìš©": 0,
            "í‰ê· _ìœ ì‚¬ë„": 0,
            "ì»¤ë²„ë¦¬ì§€": 0,
            "í‰ê°€": "ë¶€ì¡±",
            "í•´ì„": "ëª…ì‚¬ ë˜ëŠ” ê°œë…ì–´ ê°ì§€ë˜ì§€ ì•ŠìŒ"
        }


class AdvancedSentimentAnalyzer:
    """KcELECTRA ê¸°ë°˜ ê°ì • ë¶„ì„"""

    def __init__(self, use_gpu: bool = True):
        self.use_gpu = use_gpu
        self.sentiment_model = None

        # í•™ìŠµ ê´€ë ¨ í‚¤ì›Œë“œ
        self.learning_positive = ["í¥ë¯¸ë¡­", "ì¬ë¯¸ìˆ", "ì´í•´í–ˆ", "ê³µê°", "ì¸ìƒì "]
        self.learning_negative = ["ì–´ë µ", "ì´í•´ì•ˆ", "ëª¨ë¥´ê² ", "í—·ê°ˆ", "ë³µì¡"]
        self.learning_constructive = ["ê¶ê¸ˆ", "ë”ì•Œê³ ì‹¶", "ìƒê°í•´ë³¼", "íƒêµ¬"]

        self._init_model()

    def _init_model(self):
        """ê°ì • ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            from transformers import pipeline
            device = 0 if self.use_gpu else -1
            self.sentiment_model = pipeline(
                "sentiment-analysis",
                model="beomi/KcELECTRA-base-v2022",
                device=device
            )
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.sentiment_model = None

    def analyze_sentiment(self, text: str) -> Dict:
        """ë‹¤ì¸µì  ê°ì • ë¶„ì„"""
        # 1. ì „ì²´ ê°ì • (AI ëª¨ë¸)
        if self.sentiment_model:
            try:
                result = self.sentiment_model(text[:512])[0]  # ìµœëŒ€ 512ì
                overall = result['label']
                confidence = result['score']
            except Exception:
                overall, confidence = "neutral", 0.5
        else:
            overall, confidence = "neutral", 0.5

        # 2. í•™ìŠµ íƒœë„
        learning_tone = self._analyze_learning_tone(text)

        # 3. ë§¥ë½ ê°ì •
        contextual = self._contextual_sentiment(text)

        # ìµœì¢… í†µí•©
        final_tone, final_score = self._integrate_sentiments(
            overall, learning_tone, contextual
        )

        return {
            "ì „ì²´_ê°ì •": overall,
            "ì‹ ë¢°ë„": round(confidence, 3),
            "í•™ìŠµ_íƒœë„": learning_tone,
            "ìµœì¢…_í†¤": final_tone,
            "ì ìˆ˜": round(final_score, 3),
            "í•´ì„": self._interpret_tone(final_tone, learning_tone)
        }

    def _analyze_learning_tone(self, text: str) -> str:
        """í•™ìŠµ íƒœë„ ë¶„ì„"""
        pos = sum(1 for w in self.learning_positive if w in text)
        neg = sum(1 for w in self.learning_negative if w in text)
        con = sum(1 for w in self.learning_constructive if w in text)

        if con >= 2:
            return "íƒêµ¬ì "
        elif pos >= neg + 2:
            return "ì ê·¹ì "
        elif pos > neg:
            return "ê¸ì •ì "
        elif neg > pos + 2:
            return "ì†Œê·¹ì "
        else:
            return "ì¤‘ë¦½ì "

    def _contextual_sentiment(self, text: str) -> Dict:
        """ë§¥ë½ ê³ ë ¤ ê°ì •"""
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        if not sentences or not self.sentiment_model:
            return {"í‰ê· _ê°ì •": 0}

        sent_scores = []
        for sent in sentences[:5]:  # ìµœëŒ€ 5ë¬¸ì¥
            try:
                result = self.sentiment_model(sent)[0]
                label = 1 if 'positive' in result['label'].lower() else -1
                sent_scores.append(label * result['score'])
            except Exception:
                sent_scores.append(0)

        return {"í‰ê· _ê°ì •": round(float(np.mean(sent_scores)), 3)}

    def _integrate_sentiments(
        self, overall: str, learning: str, contextual: Dict
    ) -> Tuple[str, float]:
        """ê°ì • í†µí•©"""
        overall_score = 0.5 if 'positive' in overall.lower() else -0.5 if 'negative' in overall.lower() else 0
        learning_scores = {
            "íƒêµ¬ì ": 0.8, "ì ê·¹ì ": 0.6, "ê¸ì •ì ": 0.4,
            "ì¤‘ë¦½ì ": 0, "ì†Œê·¹ì ": -0.4
        }
        learning_score = learning_scores.get(learning, 0)
        contextual_score = contextual.get("í‰ê· _ê°ì •", 0)

        final_score = (
            overall_score * 0.3 +
            learning_score * 0.5 +
            contextual_score * 0.2
        )

        if final_score > 0.4:
            tone = "ë§¤ìš°ê¸ì •ì "
        elif final_score > 0.1:
            tone = "ê¸ì •ì "
        elif final_score > -0.1:
            tone = "ì¤‘ë¦½ì "
        elif final_score > -0.4:
            tone = "ë¶€ì •ì "
        else:
            tone = "ë§¤ìš°ë¶€ì •ì "

        return tone, final_score

    def _interpret_tone(self, tone: str, learning: str) -> str:
        if "ë§¤ìš°ê¸ì •" in tone and learning == "íƒêµ¬ì ":
            return "í•™ìŠµ í¥ë¯¸ì™€ íƒêµ¬ ì˜ì§€ ë†’ìŒ. ë§¤ìš° ìš°ìˆ˜í•œ í•™ìŠµ íƒœë„."
        elif "ê¸ì •" in tone:
            return "í•™ìŠµì— ì ê·¹ì . ê¸ì •ì  íƒœë„ ìœ ì§€."
        elif "ë¶€ì •" in tone:
            return "í•™ìŠµ ë™ê¸° ì €í•˜. ì§€ì› í•„ìš”."
        else:
            return "ë³´í†µ ìˆ˜ì¤€ì˜ í•™ìŠµ íƒœë„."


# ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    analyzer = ComprehensiveLanguageAnalyzer()

    test_text = """
    ì¶˜í–¥ì „ì—ì„œ ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ê²ƒì€ ë‹¹ì‹œ ì¡°ì„ ì‹œëŒ€ì˜ ì‹ ë¶„ì œì™€ ê´€ë ¨ì´ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.
    ì–‘ë°˜ê³¼ ê¸°ìƒì˜ ë”¸ì¸ ì¶˜í–¥ ì‚¬ì´ì˜ ì‚¬ë‘ì€ ê³„ê¸‰ì˜ ë²½ì„ ë„˜ì–´ì•¼ í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    ì´ê²ƒì€ ì‘í’ˆì˜ ì¤‘ìš”í•œ ì£¼ì œì¸ ì‹ ë¶„ ì°¨ë³„ì— ëŒ€í•œ ë¹„íŒì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    """

    result = analyzer.analyze(test_text)

    print("=" * 60)
    print("ğŸ“Š ì–¸ì–´ ë¶„ì„ ê²°ê³¼")
    print("=" * 60)

    import json
    print(json.dumps(result, ensure_ascii=False, indent=2))
