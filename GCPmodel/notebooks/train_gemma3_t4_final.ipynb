{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Gemma 3 9B Fine-tuning - ê³ ì „ë¬¸í•™ ì‚¬ê³ ìœ ë„ AI\n\n**ì™„ë²½í•œ í•™ìŠµ ë…¸íŠ¸ë¶ - A100 GPU ìµœì í™” ğŸš€**"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìˆ˜!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ íŒ¨í‚¤ì§€ ì œê±°\n",
    "!pip uninstall -y torch torchvision torchaudio transformers peft trl bitsandbytes numpy scipy\n",
    "\n",
    "# í˜¸í™˜ë˜ëŠ” numpy, scipy ì„¤ì¹˜\n",
    "!pip install numpy==1.26.4 scipy==1.13.1\n",
    "\n",
    "# PyTorch 2.4.0 ì„¤ì¹˜ (CUDA 12.1)\n",
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install transformers==4.44.2 peft==0.12.0 trl==0.9.6 bitsandbytes==0.43.3 datasets==2.21.0 accelerate==0.33.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâš ï¸ ì¤‘ìš”: ì§€ê¸ˆ Kernelì„ ì¬ì‹œì‘í•˜ì„¸ìš”!\")\n",
    "print(\"   Kernel â†’ Restart Kernel\")\n",
    "print(\"\\nì¬ì‹œì‘ í›„ ë‹¤ìŒ ì…€(Cell 3)ë¶€í„° ì‹¤í–‰í•˜ì„¸ìš”.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"í™˜ê²½ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: GPU ìƒì„¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"GPU ìƒíƒœ í™•ì¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nğŸ”¥ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"   ì´ ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"   Compute Capability: {props.major}.{props.minor}\")\n",
    "        \n",
    "    print(f\"\\nğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Storageì—ì„œ ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "!gsutil cp gs://knu-team-03-data/classical-literature/gemma/train_balanced.jsonl ./\n",
    "!gsutil cp gs://knu-team-03-data/classical-literature/gemma/valid_balanced.jsonl ./\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(\"   - ê³ ì „ë¬¸í•™ 50% + AI Hub ìš°ìˆ˜ ìƒ˜í”Œ 50%\")\n",
    "print(\"   - ì´ 822ê°œ ìƒ˜í”Œ (train: 657, valid: 165)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Hugging Face ë¡œê·¸ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face í† í° (ìë™ ì„¤ì •ë¨)\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ!\")\n",
    "print(\"âœ… Gemma ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 6: ë°ì´í„°ì…‹ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë¡œë“œ\ntrain_dataset = load_dataset('json', data_files='train_balanced.jsonl', split='train')\nvalid_dataset = load_dataset('json', data_files='train_balanced.jsonl', split='train')\n\nprint(\"=\"*60)\nprint(\"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\nprint(\"=\"*60)\nprint(f\"Train: {len(train_dataset)}ê°œ (ê³ ì „ 50% + AI Hub ìš°ìˆ˜ 50%)\")\nprint(f\"Valid: {len(valid_dataset)}ê°œ\")\nprint(f\"\\nâš¡ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1.5-2ì‹œê°„ (A100 40GB GPU)\")\nprint(\"=\"*60)\nprint(f\"\\nSample:\\n{train_dataset[0]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-9b-it\"\n",
    "\n",
    "# 4-bit ì–‘ìí™” ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì¤‘... (3-5ë¶„ ì†Œìš”)\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (Gemma2 ìµœì í™”: eager attention)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"  # Gemma2 ê¶Œì¥ ì„¤ì •\n",
    ")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (eager attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 8: LoRA ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì„¤ì •\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì¤€ë¹„\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LoRA ì„¤ì • ì™„ë£Œ\")\n",
    "print(\"=\"*60)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Step 9: ë°ì´í„° í¬ë§·íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ Gemma í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        text = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output}<end_of_turn>\"\"\"\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(\"=\"*60)\n",
    "print(\"ë°ì´í„° í¬ë§·íŒ… í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "print(formatting_func(train_dataset[:1])[0][:500])\n",
    "print(\"...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "## Step 10: Trainer ì„¤ì • (A100 40GB ìµœì í™” ğŸš€)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "output_dir = \"./gemma3-classical-lit-finetuned\"\n\n# A100 40GB ìµœì í™” ì„¤ì •\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    \n    # âš¡ A100 40GB - ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€!\n    per_device_train_batch_size=4,  # T4: 1 â†’ A100: 4\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,  # T4: 16 â†’ A100: 4\n    \n    # í•™ìŠµë¥  ë° ìŠ¤ì¼€ì¤„ëŸ¬\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    \n    # ë¡œê¹… ë° í‰ê°€\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    \n    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=1,\n    \n    # ì •ë°€ë„ ìµœì í™” (A100ì€ tf32 ì§€ì›!)\n    bf16=True,\n    tf32=True,  # âš¡ A100ë§Œ ê°€ëŠ¥!\n    \n    # ë©”ëª¨ë¦¬ ìµœì í™”\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    max_grad_norm=1.0,\n    \n    # ë°ì´í„° ë¡œë”© ìµœì í™”\n    dataloader_num_workers=4,  # T4: 2 â†’ A100: 4\n    dataloader_pin_memory=True,\n    \n    # ê¸°íƒ€\n    report_to=\"none\",\n    load_best_model_at_end=False,\n)\n\n# SFTTrainer ìƒì„±\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    formatting_func=formatting_func,\n    packing=False,\n    max_seq_length=1024,  # A100ì€ ë©”ëª¨ë¦¬ ì—¬ìœ  â†’ 1024\n    dataset_text_field=None,\n    tokenizer=tokenizer,\n)\n\nprint(\"=\"*60)\nprint(\"âœ… Trainer ì„¤ì • ì™„ë£Œ (A100 40GB ìµœì í™”)\")\nprint(\"=\"*60)\nprint(f\"ğŸš€ Effective batch size: 16 (per_device=4 Ã— grad_accum=4)\")\nprint(f\"ğŸš€ Max sequence length: 1024 (T4 ëŒ€ë¹„ 2ë°°!)\")\nprint(f\"âš¡ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1.5-2ì‹œê°„ (T4 ëŒ€ë¹„ 5ë°° ë¹ ë¦„!)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Step 11: í•™ìŠµ ì‹œì‘!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Step 12: ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œì»¬ ì €ì¥\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}\")\n",
    "\n",
    "# Cloud Storageì— ì—…ë¡œë“œ\n",
    "!gsutil -m cp -r {output_dir} gs://knu-team-03-data/classical-literature/models/\n",
    "print(\"\\nâœ… Cloud Storage ì—…ë¡œë“œ ì™„ë£Œ\")\n",
    "print(\"   gs://knu-team-03-data/classical-literature/models/gemma3-classical-lit-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 13: í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "test_input = \"\"\"<start_of_turn>user\n",
    "ë‹¤ìŒ ì§€ë¬¸ì„ ì½ê³  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. í•™ìƒì˜ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ë©° ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "[ì§€ë¬¸]\n",
    "ì¶˜í–¥ì „ì˜ í•œ ì¥ë©´...\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ëª¨ë¸ ì‘ë‹µ\")\n",
    "print(\"=\"*60)\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}