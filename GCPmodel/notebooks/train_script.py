#!/usr/bin/env python3
"""
Gemma 3 9B Fine-tuning - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš© ìŠ¤í¬ë¦½íŠ¸
ì™€ì´íŒŒì´ ëŠê²¨ë„ ì•ˆì „í•˜ê²Œ í•™ìŠµ ì§„í–‰
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from trl import SFTTrainer
import os

print("="*60)
print("Gemma 3 9B Fine-tuning ì‹œì‘")
print("="*60)

# í™˜ê²½ í™•ì¸
print(f"\nPyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

# ë°ì´í„°ì…‹ ë¡œë“œ
print("\në°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
train_dataset = load_dataset('json', data_files='train_balanced.jsonl', split='train')
valid_dataset = load_dataset('json', data_files='valid_balanced.jsonl', split='train')
print(f"Train: {len(train_dataset)}ê°œ, Valid: {len(valid_dataset)}ê°œ")

# Hugging Face ë¡œê·¸ì¸
from huggingface_hub import login
import os
HF_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN")
if HF_TOKEN:
    login(token=HF_TOKEN)
    print("âœ… HuggingFace ë¡œê·¸ì¸ ì™„ë£Œ")
else:
    print("âš ï¸ HUGGING_FACE_HUB_TOKEN í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")

# ëª¨ë¸ ë¡œë“œ
print("\nëª¨ë¸ ë¡œë“œ ì¤‘... (3-5ë¶„ ì†Œìš”)")
model_name = "google/gemma-2-9b-it"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="eager"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# LoRA ì„¤ì •
print("\nLoRA ì„¤ì • ì¤‘...")
peft_config = LoraConfig(
    lora_alpha=32,
    lora_dropout=0.1,
    r=16,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
print("âœ… LoRA ì„¤ì • ì™„ë£Œ")

# ë°ì´í„° í¬ë§·íŒ…
def formatting_func(examples):
    texts = []
    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):
        text = f"""<start_of_turn>user
{instruction}

{input_text}<end_of_turn>
<start_of_turn>model
{output}<end_of_turn>"""
        texts.append(text)
    return texts

# Trainer ì„¤ì •
print("\nTrainer ì„¤ì • ì¤‘...")
output_dir = "./gemma3-classical-lit-finetuned"

training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=1,
    bf16=True,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    max_grad_norm=1.0,
    dataloader_num_workers=2,
    dataloader_pin_memory=True,
    report_to="none",
    load_best_model_at_end=False,
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    formatting_func=formatting_func,
    packing=False,
    max_seq_length=512,
    dataset_text_field=None,
    tokenizer=tokenizer,
)
print("âœ… Trainer ì„¤ì • ì™„ë£Œ")

# í•™ìŠµ ì‹œì‘
print("\n" + "="*60)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*60)
trainer.train()

print("\n" + "="*60)
print("âœ… í•™ìŠµ ì™„ë£Œ!")
print("="*60)

# ëª¨ë¸ ì €ì¥
print("\nëª¨ë¸ ì €ì¥ ì¤‘...")
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"âœ… ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {output_dir}")

# Cloud Storage ì—…ë¡œë“œ
print("\nCloud Storage ì—…ë¡œë“œ ì¤‘...")
import subprocess
subprocess.run([
    "gsutil", "-m", "cp", "-r",
    output_dir,
    "gs://knu-team-03-data/classical-literature/models/"
])
print("âœ… Cloud Storage ì—…ë¡œë“œ ì™„ë£Œ!")

print("\n" + "="*60)
print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("="*60)
