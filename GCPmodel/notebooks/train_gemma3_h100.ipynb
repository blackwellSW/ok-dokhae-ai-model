{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 3 9B Fine-tuning on H100\n",
    "\n",
    "ê³ ì „ë¬¸í•™ ì‚¬ê³ ìœ ë„ AI ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í˜¸í™˜ë˜ëŠ” ë²„ì „ìœ¼ë¡œ ì„¤ì¹˜ (PyTorch 2.4.0 + Transformers 4.44.2)\n!pip uninstall -y torch torchvision torchaudio transformers peft trl bitsandbytes numpy scipy\n\n!pip install numpy==1.26.4 scipy==1.13.1\n\n!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n\n!pip install \\\n  \"transformers==4.44.2\" \\\n  \"peft==0.12.0\" \\\n  \"trl==0.9.6\" \\\n  \"bitsandbytes==0.43.3\" \\\n  \"datasets==2.21.0\" \\\n  \"accelerate==0.33.0\"\n  \nprint(\"âœ… ì„¤ì¹˜ ì™„ë£Œ! ì´ì œ Kernelì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1 GPU ìƒì„¸ í™•ì¸ (ì¤‘ìš”!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# GPU ìƒì„¸ ì •ë³´ í™•ì¸\nimport torch\n\nprint(\"=\"*60)\nprint(\"GPU ìƒíƒœ í™•ì¸\")\nprint(\"=\"*60)\n\nprint(f\"\\nâœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\nprint(f\"âœ… GPU ê°œìˆ˜: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        print(f\"\\nğŸ”¥ GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"   ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.2f} GB\")\n        print(f\"   Compute Capability: {props.major}.{props.minor}\")\n        print(f\"   ë©€í‹°í”„ë¡œì„¸ì„œ: {props.multi_processor_count}\")\n        \n    # í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n    print(f\"\\nğŸ’¾ í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©:\")\n    for i in range(torch.cuda.device_count()):\n        allocated = torch.cuda.memory_allocated(i) / 1024**3\n        reserved = torch.cuda.memory_reserved(i) / 1024**3\n        print(f\"   GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\nelse:\n    print(\"âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n    \nprint(\"\\n\" + \"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cloud Storageì—ì„œ ê· í˜•ì¡íŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n!gsutil cp gs://knu-team-03-data/classical-literature/gemma/train_balanced.jsonl ./\n!gsutil cp gs://knu-team-03-data/classical-literature/gemma/valid_balanced.jsonl ./\n\nprint(\"âœ… ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\nprint(\"   - ê³ ì „ë¬¸í•™ 50% + AI Hub ìš°ìˆ˜ ìƒ˜í”Œ 50%\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 2.1 Hugging Face ë¡œê·¸ì¸ (ì¤‘ìš”!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Hugging Face ë¡œê·¸ì¸\nfrom huggingface_hub import login\n\n# Hugging Face í† í°\nHF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n\nlogin(token=HF_TOKEN)\nprint(\"âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ!\")\nprint(\"âœ… Gemma ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ë¨\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë¡œë“œ\ntrain_dataset = load_dataset('json', data_files='train_balanced.jsonl', split='train')\nvalid_dataset = load_dataset('json', data_files='valid_balanced.jsonl', split='train')\n\nprint(f\"âœ… ê· í˜•ì¡íŒ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ!\")\nprint(f\"   Train: {len(train_dataset)}ê°œ (ê³ ì „ 50% + AI Hub ìš°ìˆ˜ 50%)\")\nprint(f\"   Valid: {len(valid_dataset)}ê°œ\")\nprint(f\"\\nâš¡ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 30-40ë¶„ (ê¸°ì¡´ 6ì‹œê°„ì˜ 1/10)\")\nprint(f\"\\nSample: {train_dataset[0]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model_name = \"google/gemma-2-9b-it\"\n\n# 4-bit ì–‘ìí™” ì„¤ì •\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# ëª¨ë¸ ë¡œë“œ (Gemma2 ìµœì í™”: eager attention)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # Gemma2 ê¶Œì¥ ì„¤ì •\n)\n\n# í† í¬ë‚˜ì´ì € ë¡œë“œ\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (eager attention)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì„¤ì •\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì¤€ë¹„\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë°ì´í„° í¬ë§·íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(examples):\n",
    "    \"\"\"ë°ì´í„°ë¥¼ Gemma í˜•ì‹ìœ¼ë¡œ ë³€í™˜\"\"\"\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        text = f\"\"\"<start_of_turn>user\n",
    "{instruction}\n",
    "\n",
    "{input_text}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{output}<end_of_turn>\"\"\"\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "print(formatting_func(train_dataset[:1])[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# âš ï¸ ì´ ì…€ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ì…€(Cell 18)ì„ ì‚¬ìš©í•˜ì„¸ìš”!\n# ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •ì€ Cell 18ì— ìˆìŠµë‹ˆë‹¤.\n\nprint(\"âš ï¸ ì´ ì…€ì„ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš”!\")\nprint(\"âš ï¸ ëŒ€ì‹  Cell 18 (ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „)ì„ ì‹¤í–‰í•˜ì„¸ìš”!\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "output_dir = \"./gemma3-classical-lit-finetuned\"\n\n# A100 40GB ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    \n    # âš¡ A100 40GB ë©”ëª¨ë¦¬ì— ë§ì¶˜ ë°°ì¹˜ í¬ê¸°\n    per_device_train_batch_size=1,  # 4â†’1ë¡œ ê°ì†Œ (ë©”ëª¨ë¦¬ ë¶€ì¡±)\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=16,  # 4â†’16ìœ¼ë¡œ ì¦ê°€ (effective batch=16 ìœ ì§€)\n    \n    # í•™ìŠµë¥  ë° ìŠ¤ì¼€ì¤„ëŸ¬\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.03,\n    \n    # ë¡œê¹… ë° í‰ê°€\n    logging_steps=10,\n    eval_strategy=\"steps\",\n    eval_steps=50,  # 200â†’50 (ë” ìì£¼ ì²´í¬)\n    \n    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=1,  # 2â†’1 (ë””ìŠ¤í¬ ì ˆì•½)\n    \n    # ì •ë°€ë„ ìµœì í™”\n    bf16=True,\n    tf32=True,\n    \n    # ë©”ëª¨ë¦¬ ìµœì í™” (ì¤‘ìš”!)\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    max_grad_norm=1.0,\n    \n    # ë°ì´í„° ë¡œë”©\n    dataloader_num_workers=2,  # 4â†’2 (ë©”ëª¨ë¦¬ ì ˆì•½)\n    dataloader_pin_memory=True,\n    \n    # ê¸°íƒ€\n    report_to=\"none\",\n    load_best_model_at_end=False,\n)\n\nfrom trl import SFTTrainer\n\n# SFTTrainer \ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    formatting_func=formatting_func,\n    packing=False,\n    max_seq_length=512,  # 1024â†’512 (ë©”ëª¨ë¦¬ ì ˆì•½!)\n    dataset_text_field=None,\n    tokenizer=tokenizer,\n)\n\nprint(\"âœ… Trainer ì„¤ì • ì™„ë£Œ (A100 40GB ë©”ëª¨ë¦¬ ìµœì í™”)\")\nprint(f\"   Effective batch size: 16 (per_device=1 Ã— grad_accum=16)\")\nprint(f\"   Max sequence length: 512\")\nprint(f\"   ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 40-60ë¶„\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "trainer.train()\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œì»¬ ì €ì¥\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}\")\n",
    "\n",
    "# Cloud Storageì— ì—…ë¡œë“œ\n",
    "!gsutil -m cp -r {output_dir} gs://knu-team-03-data/classical-literature/models/\n",
    "print(\"âœ… Cloud Storage ì—…ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "test_input = \"\"\"<start_of_turn>user\n",
    "ë‹¤ìŒ ì§€ë¬¸ì„ ì½ê³  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. í•™ìƒì˜ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ë©° ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "[ì§€ë¬¸]\n",
    "ì¶˜í–¥ì „ì˜ í•œ ì¥ë©´...\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}