#!/usr/bin/env python3
"""
Vertex AI Training Pipelineìš© í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
"""

# Gemma 3 ì§€ì›ì„ ìœ„í•´ ìµœì‹  íŒ¨í‚¤ì§€ ì„¤ì¹˜
import subprocess
import sys
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "--upgrade",
                       "transformers>=4.49.0", "accelerate>=0.34.0", "peft>=0.14.0", "trl>=0.12.0", "bitsandbytes>=0.46.1"])

import os
import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from trl import SFTTrainer
from huggingface_hub import login
from google.cloud import storage

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--train-data", type=str, required=True, help="GCS path to train data")
    parser.add_argument("--valid-data", type=str, required=True, help="GCS path to valid data")
    parser.add_argument("--output-dir", type=str, required=True, help="GCS path to output directory")
    parser.add_argument("--hf-token", type=str, required=True, help="HuggingFace token")
    parser.add_argument("--model-name", type=str, default="google/gemma-3-12b-it")
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--grad-accum", type=int, default=4)
    parser.add_argument("--learning-rate", type=float, default=2e-4)
    parser.add_argument("--max-seq-length", type=int, default=1024)
    return parser.parse_args()

def download_from_gcs(gcs_path, local_path):
    """GCSì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    if not gcs_path.startswith("gs://"):
        return gcs_path

    bucket_name = gcs_path.split("/")[2]
    blob_path = "/".join(gcs_path.split("/")[3:])

    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    blob.download_to_filename(local_path)

    return local_path

def formatting_func(examples):
    """Gemma í˜•ì‹ìœ¼ë¡œ ë°ì´í„° í¬ë§·íŒ… (input_text, output_text í˜•ì‹ ì§€ì›)"""
    texts = []

    # ìƒˆë¡œìš´ í˜•ì‹ (input_text, output_text)
    if 'input_text' in examples:
        for input_text, output_text in zip(examples['input_text'], examples['output_text']):
            text = f"""<start_of_turn>user
{input_text}<end_of_turn>
<start_of_turn>model
{output_text}<end_of_turn>"""
            texts.append(text)
    # ê¸°ì¡´ í˜•ì‹ (instruction, input, output)
    else:
        for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):
            text = f"""<start_of_turn>user
{instruction}

{input_text}<end_of_turn>
<start_of_turn>model
{output}<end_of_turn>"""
            texts.append(text)
    return texts

def main():
    args = parse_args()

    print("=" * 60)
    print("ğŸš€ Vertex AI Training Pipeline - Gemma 3 Fine-tuning")
    print("=" * 60)
    print(f"Model: {args.model_name}")
    print(f"Train data: {args.train_data}")
    print(f"Output: {args.output_dir}")
    print("=" * 60)

    # HuggingFace ë¡œê·¸ì¸
    print("\nğŸ” HuggingFace ë¡œê·¸ì¸...")
    login(token=args.hf_token)
    print("âœ… ë¡œê·¸ì¸ ì™„ë£Œ")

    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    print("\nğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
    train_local = "/tmp/train_balanced.jsonl"
    valid_local = "/tmp/valid_balanced.jsonl"

    download_from_gcs(args.train_data, train_local)
    download_from_gcs(args.valid_data, valid_local)

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ...")
    train_dataset = load_dataset('json', data_files=train_local, split='train')
    valid_dataset = load_dataset('json', data_files=valid_local, split='train')
    print(f"âœ… Train: {len(train_dataset)}, Valid: {len(valid_dataset)}")

    # GPU í™•ì¸
    print("\nğŸ”¥ GPU í™•ì¸...")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="eager"
    )

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # LoRA ì„¤ì •
    print("\nğŸ”§ LoRA ì„¤ì •...")
    peft_config = LoraConfig(
        lora_alpha=32,
        lora_dropout=0.1,
        r=16,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )

    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # Trainer ì„¤ì •
    print("\nâš™ï¸ Trainer ì„¤ì •...")
    local_output_dir = "/tmp/output"
    os.makedirs(local_output_dir, exist_ok=True)

    training_args = TrainingArguments(
        output_dir=local_output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=500,
        save_total_limit=2,
        bf16=True,
        tf32=True,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        max_grad_norm=1.0,
        dataloader_num_workers=4,
        dataloader_pin_memory=True,
        report_to="none",
        load_best_model_at_end=False,
    )

    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        formatting_func=formatting_func,
        packing=False,
        max_seq_length=args.max_seq_length,
        dataset_text_field=None,
        tokenizer=tokenizer,
    )

    # í•™ìŠµ ì‹œì‘
    print("\n" + "=" * 60)
    print("ğŸš€ í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    trainer.train()

    # ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    trainer.save_model(local_output_dir)
    tokenizer.save_pretrained(local_output_dir)
    print(f"âœ… ë¡œì»¬ ì €ì¥ ì™„ë£Œ: {local_output_dir}")

    # GCSì— ì—…ë¡œë“œ
    print(f"\nğŸ“¤ GCS ì—…ë¡œë“œ ì¤‘: {args.output_dir}")
    import subprocess
    subprocess.run([
        "gsutil", "-m", "cp", "-r",
        f"{local_output_dir}/*",
        args.output_dir
    ], check=True)

    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {args.output_dir}")

if __name__ == "__main__":
    main()
