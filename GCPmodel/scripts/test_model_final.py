#!/usr/bin/env python3
"""
íŠœë‹ëœ ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
REST APIë¥¼ í†µí•œ ì§ì ‘ í˜¸ì¶œ
"""

import argparse
import json
import time
import requests
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import subprocess


def get_access_token() -> str:
    """GCP Access Token íšë“"""
    result = subprocess.run(
        ["gcloud", "auth", "application-default", "print-access-token"],
        capture_output=True,
        text=True
    )
    return result.stdout.strip()


def test_tuned_model(
    endpoint_id: str,
    project_number: str,
    location: str = "us-central1",
    test_prompts: List[Dict] = None
) -> List[Dict]:
    """
    íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸

    Args:
        endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID
        project_number: GCP í”„ë¡œì íŠ¸ ë²ˆí˜¸
        location: ë¦¬ì „
        test_prompts: í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª íŠœë‹ëœ ëª¨ë¸ ìµœì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"ì—”ë“œí¬ì¸íŠ¸ ID: {endpoint_id}")
    print(f"í”„ë¡œì íŠ¸ ë²ˆí˜¸: {project_number}")
    print(f"ë¦¬ì „: {location}")
    print("-" * 70)

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    if test_prompts is None:
        test_prompts = [
            {
                "name": "ì¶˜í–¥ì „_ì§ˆë¬¸1",
                "student_input": "ì¶˜í–¥ì „ì—ì„œ ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ê°€ ë­”ê°€ìš”?",
                "context": "ì¶˜í–¥ì „"
            },
            {
                "name": "ì‹¬ì²­ì „_ì§ˆë¬¸1",
                "student_input": "ì‹¬ì²­ì´ëŠ” ì™œ ì¸ë‹¹ìˆ˜ì— ëª¸ì„ ë˜ì¡Œë‚˜ìš”?",
                "context": "ì‹¬ì²­ì „"
            },
            {
                "name": "í¥ë¶€ì „_ì§ˆë¬¸1",
                "student_input": "í¥ë¶€ì™€ ë†€ë¶€ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "context": "í¥ë¶€ì „"
            },
            {
                "name": "í‘œí˜„_ì§ˆë¬¸",
                "student_input": "ì˜ì¸í™” ê¸°ë²•ì´ ë­”ê°€ìš”?",
                "context": "ê³ ì „ë¬¸í•™"
            },
            {
                "name": "ì£¼ì œ_ì§ˆë¬¸",
                "student_input": "ì¶˜í–¥ì „ì˜ ì£¼ì œê°€ ë­ì˜ˆìš”?",
                "context": "ì¶˜í–¥ì „"
            },
            {
                "name": "ë¹„êµ_ì§ˆë¬¸",
                "student_input": "ì¶˜í–¥ì „ê³¼ ì‹¬ì²­ì „ì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "context": "ê³ ì „ë¬¸í•™"
            }
        ]

    results = []

    # Access Token íšë“
    print("ğŸ”‘ Access Token íšë“ ì¤‘...")
    access_token = get_access_token()
    print("âœ… Access Token íšë“ ì™„ë£Œ\n")

    # API ì—”ë“œí¬ì¸íŠ¸
    api_url = f"https://{location}-aiplatform.googleapis.com/v1/projects/{project_number}/locations/{location}/endpoints/{endpoint_id}:generateContent"

    # ê° í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰
    for i, test_case in enumerate(test_prompts, 1):
        print(f"\n{'='*70}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}/{len(test_prompts)}: {test_case['name']}")
        print(f"{'='*70}")
        print(f"ë§¥ë½: {test_case['context']}")
        print(f"ì§ˆë¬¸: {test_case['student_input']}")
        print("-" * 70)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = construct_prompt(test_case['student_input'], test_case['context'])

        # ìš”ì²­ ë³¸ë¬¸
        request_body = {
            "contents": {
                "role": "user",
                "parts": {
                    "text": prompt
                }
            },
            "generation_config": {
                "temperature": 0.7,
                "maxOutputTokens": 512,
                "topP": 0.9
            }
        }

        # í—¤ë”
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }

        # ì¶”ë¡  ì‹¤í–‰ (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()

        try:
            response = requests.post(api_url, json=request_body, headers=headers, timeout=30)
            inference_time = time.time() - start_time

            if response.status_code == 200:
                response_json = response.json()

                # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                response_text = ""
                if "candidates" in response_json and len(response_json["candidates"]) > 0:
                    candidate = response_json["candidates"][0]
                    if "content" in candidate and "parts" in candidate["content"]:
                        parts = candidate["content"]["parts"]
                        if len(parts) > 0 and "text" in parts[0]:
                            response_text = parts[0]["text"]

                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                usage_metadata = response_json.get("usageMetadata", {})
                prompt_tokens = usage_metadata.get("promptTokenCount", 0)
                output_tokens = usage_metadata.get("candidatesTokenCount", 0)
                total_tokens = usage_metadata.get("totalTokenCount", 0)

                # ê²°ê³¼ ë¶„ì„
                result = analyze_response(
                    test_case=test_case,
                    response=response_text,
                    inference_time=inference_time,
                    prompt_tokens=prompt_tokens,
                    output_tokens=output_tokens,
                    total_tokens=total_tokens
                )

                results.append(result)

                # ê²°ê³¼ ì¶œë ¥
                print_test_result(result)

            else:
                print(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                print(f"   ì—ëŸ¬: {response.text}")
                results.append({
                    "test_name": test_case['name'],
                    "status": "failed",
                    "error": f"HTTP {response.status_code}: {response.text}"
                })

        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            results.append({
                "test_name": test_case['name'],
                "status": "failed",
                "error": str(e)
            })

        print("-" * 70)
        time.sleep(1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€

    return results


def construct_prompt(student_input: str, context: str = None) -> str:
    """í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ë°ì´í„°ì…‹ê³¼ ë™ì¼í•œ í˜•ì‹"""
    # í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
    if context:
        return f"""[ì‘í’ˆ: {context}]
í•™ìƒ: {student_input}

êµì‚¬: """
    else:
        return f"""í•™ìƒ: {student_input}

êµì‚¬: """


def analyze_response(
    test_case: Dict,
    response: str,
    inference_time: float,
    prompt_tokens: int,
    output_tokens: int,
    total_tokens: int
) -> Dict:
    """ì‘ë‹µ ë¶„ì„"""
    # íƒœê·¸ ì¡´ì¬ í™•ì¸
    has_induction_tag = "[ì‚¬ê³ ìœ ë„]" in response
    has_log_tag = "[ì‚¬ê³ ë¡œê·¸]" in response

    # íƒœê·¸ ë‚´ìš© ì¶”ì¶œ
    induction_content = extract_tag_content(response, "ì‚¬ê³ ìœ ë„")
    log_content = extract_tag_content(response, "ì‚¬ê³ ë¡œê·¸")

    # í† í° ì²˜ë¦¬ ì†ë„
    tokens_per_second = output_tokens / inference_time if inference_time > 0 else 0

    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = 0

    # 1. íƒœê·¸ ì‚¬ìš© (50ì )
    if has_induction_tag:
        quality_score += 30
    if has_log_tag:
        quality_score += 20

    # 2. ë‚´ìš© ì¶©ì‹¤ë„ (30ì )
    if len(induction_content) > 50:
        quality_score += 15
    if len(log_content) > 20:
        quality_score += 15

    # 3. ì§ˆë¬¸ í¬í•¨ ì—¬ë¶€ (ì‚¬ê³ ìœ ë„) (20ì )
    question_count = induction_content.count("?")
    if question_count >= 1:
        quality_score += 10
    if question_count >= 2:
        quality_score += 10

    return {
        "test_name": test_case["name"],
        "status": "success",
        "inference_time": round(inference_time, 3),
        "prompt_tokens": prompt_tokens,
        "output_tokens": output_tokens,
        "total_tokens": total_tokens,
        "tokens_per_second": round(tokens_per_second, 2),
        "has_induction_tag": has_induction_tag,
        "has_log_tag": has_log_tag,
        "induction_length": len(induction_content),
        "log_length": len(log_content),
        "question_count": question_count,
        "quality_score": quality_score,
        "response": response,
        "induction_content": induction_content,
        "log_content": log_content
    }


def extract_tag_content(text: str, tag: str) -> str:
    """íƒœê·¸ ë‚´ìš© ì¶”ì¶œ"""
    import re
    pattern = rf"\[{tag}\]\s*(.*?)(?=\[|$)"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else ""


def print_test_result(result: Dict):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
    if result["status"] == "failed":
        print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        return

    print(f"â±ï¸  ì¶”ë¡  ì‹œê°„: {result['inference_time']}ì´ˆ")
    print(f"ğŸ“Š í† í°: {result['prompt_tokens']} (ì…ë ¥) + {result['output_tokens']} (ì¶œë ¥) = {result['total_tokens']}")
    print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {result['tokens_per_second']} tokens/sec")
    print(f"âœ… [ì‚¬ê³ ìœ ë„] íƒœê·¸: {'âœ“' if result['has_induction_tag'] else 'âœ—'}")
    print(f"âœ… [ì‚¬ê³ ë¡œê·¸] íƒœê·¸: {'âœ“' if result['has_log_tag'] else 'âœ—'}")
    print(f"ğŸ“ ì‚¬ê³ ìœ ë„ ê¸¸ì´: {result['induction_length']} ì (ì§ˆë¬¸ {result['question_count']}ê°œ)")
    print(f"ğŸ“ ì‚¬ê³ ë¡œê·¸ ê¸¸ì´: {result['log_length']} ì")
    print(f"â­ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/100")

    print(f"\nğŸ’¬ ì‘ë‹µ ë‚´ìš©:")
    print("-" * 70)
    print(result['response'])
    print("-" * 70)


def generate_performance_report(results: List[Dict], output_path: str = None):
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
    print("=" * 70)

    # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ì§‘ê³„
    successful_tests = [r for r in results if r.get("status") == "success"]

    if not successful_tests:
        print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_inference_time = sum(r["inference_time"] for r in successful_tests) / len(successful_tests)
    avg_tokens_per_sec = sum(r["tokens_per_second"] for r in successful_tests) / len(successful_tests)
    avg_quality_score = sum(r["quality_score"] for r in successful_tests) / len(successful_tests)
    avg_prompt_tokens = sum(r["prompt_tokens"] for r in successful_tests) / len(successful_tests)
    avg_output_tokens = sum(r["output_tokens"] for r in successful_tests) / len(successful_tests)

    # íƒœê·¸ ì‚¬ìš©ë¥ 
    induction_tag_rate = sum(1 for r in successful_tests if r["has_induction_tag"]) / len(successful_tests) * 100
    log_tag_rate = sum(1 for r in successful_tests if r["has_log_tag"]) / len(successful_tests) * 100

    print(f"\nâœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}/{len(results)}")
    print(f"â±ï¸  í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.3f}ì´ˆ")
    print(f"ğŸ“Š í‰ê·  í† í°: {avg_prompt_tokens:.0f} (ì…ë ¥) + {avg_output_tokens:.0f} (ì¶œë ¥)")
    print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_tokens_per_sec:.2f} tokens/sec")
    print(f"â­ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality_score:.1f}/100")
    print(f"âœ… [ì‚¬ê³ ìœ ë„] íƒœê·¸ ì‚¬ìš©ë¥ : {induction_tag_rate:.1f}%")
    print(f"âœ… [ì‚¬ê³ ë¡œê·¸] íƒœê·¸ ì‚¬ìš©ë¥ : {log_tag_rate:.1f}%")

    # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
    print("\n" + "-" * 90)
    print("ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print("-" * 90)
    print(f"{'í…ŒìŠ¤íŠ¸ëª…':<25} {'ì‹œê°„(ì´ˆ)':<10} {'í’ˆì§ˆ':<10} {'ì‚¬ê³ ìœ ë„':<10} {'ì‚¬ê³ ë¡œê·¸':<10}")
    print("-" * 90)

    for r in successful_tests:
        print(f"{r['test_name']:<25} {r['inference_time']:<10.3f} {r['quality_score']:<10} "
              f"{'âœ“' if r['has_induction_tag'] else 'âœ—':<10} {'âœ“' if r['has_log_tag'] else 'âœ—':<10}")

    print("=" * 90)

    # ê²°ê³¼ ì €ì¥
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(results) - len(successful_tests),
                "avg_inference_time": round(avg_inference_time, 3),
                "avg_prompt_tokens": round(avg_prompt_tokens, 1),
                "avg_output_tokens": round(avg_output_tokens, 1),
                "avg_tokens_per_second": round(avg_tokens_per_sec, 2),
                "avg_quality_score": round(avg_quality_score, 1),
                "induction_tag_rate": round(induction_tag_rate, 1),
                "log_tag_rate": round(log_tag_rate, 1)
            },
            "detailed_results": results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")

    parser.add_argument(
        "--endpoint-id",
        type=str,
        default="479737813919596544",
        help="ì—”ë“œí¬ì¸íŠ¸ ID"
    )
    parser.add_argument(
        "--project-number",
        type=str,
        default="84537953160",
        help="GCP í”„ë¡œì íŠ¸ ë²ˆí˜¸"
    )
    parser.add_argument(
        "--location",
        type=str,
        default="us-central1",
        help="Vertex AI ë¦¬ì „"
    )
    parser.add_argument(
        "--test-prompts",
        type=str,
        default="",
        help="ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ (JSON)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/performance_test_results.json",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )

    args = parser.parse_args()

    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    test_prompts = None
    if args.test_prompts and Path(args.test_prompts).exists():
        with open(args.test_prompts, 'r', encoding='utf-8') as f:
            test_prompts = json.load(f)

    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = test_tuned_model(
        endpoint_id=args.endpoint_id,
        project_number=args.project_number,
        location=args.location,
        test_prompts=test_prompts
    )

    # ë¦¬í¬íŠ¸ ìƒì„±
    if results:
        generate_performance_report(results, args.output)
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
