#!/usr/bin/env python3
"""
ë°°í¬ëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Vertex AI Endpointë¥¼ í†µí•œ ì¶”ë¡  ì„±ëŠ¥ ë° í’ˆì§ˆ í‰ê°€
"""

import argparse
import json
import time
from pathlib import Path
from typing import Dict, List
from datetime import datetime

from google.cloud import aiplatform


def test_endpoint_inference(
    endpoint_id: str,
    project_id: str,
    location: str = "us-central1",
    test_prompts: List[Dict] = None
) -> List[Dict]:
    """
    ë°°í¬ëœ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸

    Args:
        endpoint_id: ì—”ë“œí¬ì¸íŠ¸ ID
        project_id: GCP í”„ë¡œì íŠ¸ ID
        location: ë¦¬ì „
        test_prompts: í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸

    Returns:
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    print("\n" + "=" * 70)
    print("ğŸ§ª ë°°í¬ëœ ëª¨ë¸ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"ì—”ë“œí¬ì¸íŠ¸ ID: {endpoint_id}")
    print(f"í”„ë¡œì íŠ¸: {project_id}")
    print(f"ë¦¬ì „: {location}")
    print("-" * 70)

    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    if test_prompts is None:
        test_prompts = [
            {
                "name": "ì¶˜í–¥ì „_ì§ˆë¬¸1",
                "context": "ì¶˜í–¥ì „",
                "student_input": "ì¶˜í–¥ì „ì—ì„œ ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ê°€ ë­”ê°€ìš”?",
                "expected_tags": ["[ì‚¬ê³ ìœ ë„]", "[ì‚¬ê³ ë¡œê·¸]"]
            },
            {
                "name": "ì‹¬ì²­ì „_ì§ˆë¬¸1",
                "context": "ì‹¬ì²­ì „",
                "student_input": "ì‹¬ì²­ì´ëŠ” ì™œ ì¸ë‹¹ìˆ˜ì— ëª¸ì„ ë˜ì¡Œë‚˜ìš”?",
                "expected_tags": ["[ì‚¬ê³ ìœ ë„]", "[ì‚¬ê³ ë¡œê·¸]"]
            },
            {
                "name": "í¥ë¶€ì „_ì§ˆë¬¸1",
                "context": "í¥ë¶€ì „",
                "student_input": "í¥ë¶€ì™€ ë†€ë¶€ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "expected_tags": ["[ì‚¬ê³ ìœ ë„]", "[ì‚¬ê³ ë¡œê·¸]"]
            },
            {
                "name": "í‘œí˜„_ì§ˆë¬¸",
                "context": "ê³ ì „ë¬¸í•™",
                "student_input": "ì˜ì¸í™” ê¸°ë²•ì´ ë­”ê°€ìš”?",
                "expected_tags": ["[ì‚¬ê³ ìœ ë„]", "[ì‚¬ê³ ë¡œê·¸]"]
            },
            {
                "name": "ì£¼ì œ_ì§ˆë¬¸",
                "context": "ì¶˜í–¥ì „",
                "student_input": "ì¶˜í–¥ì „ì˜ ì£¼ì œê°€ ë­ì˜ˆìš”?",
                "expected_tags": ["[ì‚¬ê³ ìœ ë„]", "[ì‚¬ê³ ë¡œê·¸]"]
            }
        ]

    results = []

    # Vertex AI ì´ˆê¸°í™”
    aiplatform.init(project=project_id, location=location)
    print(f"âœ… Vertex AI ì´ˆê¸°í™” ì™„ë£Œ")

    # ì—”ë“œí¬ì¸íŠ¸ ë¡œë“œ
    try:
        endpoint = aiplatform.Endpoint(endpoint_id)
        print(f"âœ… ì—”ë“œí¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ ì—”ë“œí¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

    # ê° í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰
    for i, test_case in enumerate(test_prompts, 1):
        print(f"\n{'='*70}")
        print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}/{len(test_prompts)}: {test_case['name']}")
        print(f"{'='*70}")
        print(f"ë§¥ë½: {test_case['context']}")
        print(f"ì§ˆë¬¸: {test_case['student_input']}")
        print("-" * 70)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = construct_prompt(test_case['student_input'], test_case['context'])

        # ìš”ì²­ ì¸ìŠ¤í„´ìŠ¤ êµ¬ì„±
        instances = [{"prompt": prompt}]
        parameters = {
            "max_output_tokens": 512,
            "temperature": 0.7,
            "top_p": 0.9,
        }

        # ì¶”ë¡  ì‹¤í–‰ (ì‹œê°„ ì¸¡ì •)
        start_time = time.time()

        try:
            predictions = endpoint.predict(instances=instances, parameters=parameters)
            inference_time = time.time() - start_time

            # ì‘ë‹µ ì¶”ì¶œ
            if predictions.predictions:
                response_text = predictions.predictions[0]
                # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° content í‚¤ ì¶”ì¶œ
                if isinstance(response_text, dict):
                    response_text = response_text.get('content', str(response_text))
            else:
                response_text = ""

            # ê²°ê³¼ ë¶„ì„
            result = analyze_response(
                test_case=test_case,
                response=response_text,
                inference_time=inference_time
            )

            results.append(result)

            # ê²°ê³¼ ì¶œë ¥
            print_test_result(result)

        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
            results.append({
                "test_name": test_case['name'],
                "status": "failed",
                "error": str(e)
            })

        print("-" * 70)
        time.sleep(1)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€

    return results


def construct_prompt(student_input: str, context: str = None) -> str:
    """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    system_prompt = """í•™ìƒì˜ ì‚¬ê³ ë¥¼ ìœ ë„í•˜ë©° ê³ ì „ë¬¸í•™ì„ ê°€ë¥´ì¹˜ì„¸ìš”. [ì‚¬ê³ ìœ ë„]ì™€ [ì‚¬ê³ ë¡œê·¸] íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

[ì‚¬ê³ ìœ ë„]: í•™ìƒì´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ì  ì§ˆë¬¸ì„ ì œì‹œí•©ë‹ˆë‹¤.
[ì‚¬ê³ ë¡œê·¸]: í•™ìƒì˜ ì‚¬ê³  ê³¼ì •ì„ ê´€ì°°í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤."""

    if context:
        return f"""{system_prompt}

[ë§¥ë½]
{context}

í•™ìƒ: {student_input}

AI: [ì‚¬ê³ ìœ ë„]"""
    else:
        return f"""{system_prompt}

í•™ìƒ: {student_input}

AI: [ì‚¬ê³ ìœ ë„]"""


def analyze_response(test_case: Dict, response: str, inference_time: float) -> Dict:
    """ì‘ë‹µ ë¶„ì„"""
    # íƒœê·¸ ì¡´ì¬ í™•ì¸
    has_induction_tag = "[ì‚¬ê³ ìœ ë„]" in response
    has_log_tag = "[ì‚¬ê³ ë¡œê·¸]" in response

    # íƒœê·¸ ë‚´ìš© ì¶”ì¶œ
    induction_content = extract_tag_content(response, "ì‚¬ê³ ìœ ë„")
    log_content = extract_tag_content(response, "ì‚¬ê³ ë¡œê·¸")

    # í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )
    token_count = len(response.split())
    tokens_per_second = token_count / inference_time if inference_time > 0 else 0

    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    quality_score = 0
    if has_induction_tag:
        quality_score += 30
    if has_log_tag:
        quality_score += 20
    if len(induction_content) > 50:
        quality_score += 25
    if "?" in induction_content:  # ì§ˆë¬¸ í¬í•¨ ì—¬ë¶€
        quality_score += 15
    if len(log_content) > 20:
        quality_score += 10

    return {
        "test_name": test_case["name"],
        "status": "success",
        "inference_time": round(inference_time, 3),
        "token_count": token_count,
        "tokens_per_second": round(tokens_per_second, 2),
        "has_induction_tag": has_induction_tag,
        "has_log_tag": has_log_tag,
        "induction_length": len(induction_content),
        "log_length": len(log_content),
        "quality_score": quality_score,
        "response": response,
        "induction_content": induction_content,
        "log_content": log_content
    }


def extract_tag_content(text: str, tag: str) -> str:
    """íƒœê·¸ ë‚´ìš© ì¶”ì¶œ"""
    import re
    pattern = rf"\[{tag}\]\s*(.*?)(?=\[|$)"
    match = re.search(pattern, text, re.DOTALL)
    return match.group(1).strip() if match else ""


def print_test_result(result: Dict):
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
    if result["status"] == "failed":
        print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        return

    print(f"â±ï¸  ì¶”ë¡  ì‹œê°„: {result['inference_time']}ì´ˆ")
    print(f"ğŸ“Š í† í° ìˆ˜: {result['token_count']} ({result['tokens_per_second']} tokens/sec)")
    print(f"âœ… [ì‚¬ê³ ìœ ë„] íƒœê·¸: {'ìˆìŒ' if result['has_induction_tag'] else 'ì—†ìŒ'}")
    print(f"âœ… [ì‚¬ê³ ë¡œê·¸] íƒœê·¸: {'ìˆìŒ' if result['has_log_tag'] else 'ì—†ìŒ'}")
    print(f"ğŸ“ ì‚¬ê³ ìœ ë„ ê¸¸ì´: {result['induction_length']} ì")
    print(f"ğŸ“ ì‚¬ê³ ë¡œê·¸ ê¸¸ì´: {result['log_length']} ì")
    print(f"â­ í’ˆì§ˆ ì ìˆ˜: {result['quality_score']}/100")

    print(f"\nğŸ’¬ ì‘ë‹µ ë‚´ìš©:")
    print("-" * 70)
    print(result['response'][:500])
    if len(result['response']) > 500:
        print("\n... (ìƒëµ) ...")


def generate_performance_report(results: List[Dict], output_path: str = None):
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
    print("=" * 70)

    # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ì§‘ê³„
    successful_tests = [r for r in results if r.get("status") == "success"]

    if not successful_tests:
        print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°
    avg_inference_time = sum(r["inference_time"] for r in successful_tests) / len(successful_tests)
    avg_tokens_per_sec = sum(r["tokens_per_second"] for r in successful_tests) / len(successful_tests)
    avg_quality_score = sum(r["quality_score"] for r in successful_tests) / len(successful_tests)

    # íƒœê·¸ ì‚¬ìš©ë¥ 
    induction_tag_rate = sum(1 for r in successful_tests if r["has_induction_tag"]) / len(successful_tests) * 100
    log_tag_rate = sum(1 for r in successful_tests if r["has_log_tag"]) / len(successful_tests) * 100

    print(f"\nâœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {len(successful_tests)}/{len(results)}")
    print(f"â±ï¸  í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time:.3f}ì´ˆ")
    print(f"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: {avg_tokens_per_sec:.2f} tokens/sec")
    print(f"â­ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality_score:.1f}/100")
    print(f"âœ… [ì‚¬ê³ ìœ ë„] íƒœê·¸ ì‚¬ìš©ë¥ : {induction_tag_rate:.1f}%")
    print(f"âœ… [ì‚¬ê³ ë¡œê·¸] íƒœê·¸ ì‚¬ìš©ë¥ : {log_tag_rate:.1f}%")

    # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
    print("\n" + "-" * 70)
    print("ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print("-" * 70)
    print(f"{'í…ŒìŠ¤íŠ¸ëª…':<20} {'ì‹œê°„(ì´ˆ)':<12} {'í’ˆì§ˆì ìˆ˜':<12} {'íƒœê·¸':<10}")
    print("-" * 70)

    for r in successful_tests:
        tags = ""
        if r["has_induction_tag"]:
            tags += "ğŸŸ¢"
        else:
            tags += "ğŸ”´"
        if r["has_log_tag"]:
            tags += "ğŸŸ¢"
        else:
            tags += "ğŸ”´"

        print(f"{r['test_name']:<20} {r['inference_time']:<12.3f} {r['quality_score']:<12} {tags:<10}")

    print("=" * 70)

    # ê²°ê³¼ ì €ì¥
    if output_path:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(results) - len(successful_tests),
                "avg_inference_time": round(avg_inference_time, 3),
                "avg_tokens_per_second": round(avg_tokens_per_sec, 2),
                "avg_quality_score": round(avg_quality_score, 1),
                "induction_tag_rate": round(induction_tag_rate, 1),
                "log_tag_rate": round(log_tag_rate, 1)
            },
            "detailed_results": results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="ë°°í¬ëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")

    parser.add_argument(
        "--endpoint-id",
        type=str,
        default="479737813919596544",  # ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ (checkpoint 3)
        help="ì—”ë“œí¬ì¸íŠ¸ ID"
    )
    parser.add_argument(
        "--project-id",
        type=str,
        default="knu-team-03",
        help="GCP í”„ë¡œì íŠ¸ ID"
    )
    parser.add_argument(
        "--location",
        type=str,
        default="us-central1",
        help="Vertex AI ë¦¬ì „"
    )
    parser.add_argument(
        "--test-prompts",
        type=str,
        default="",
        help="ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼ (JSON)"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="outputs/performance_test_results.json",
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ"
    )

    args = parser.parse_args()

    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    test_prompts = None
    if args.test_prompts and Path(args.test_prompts).exists():
        with open(args.test_prompts, 'r', encoding='utf-8') as f:
            test_prompts = json.load(f)

    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = test_endpoint_inference(
        endpoint_id=args.endpoint_id,
        project_id=args.project_id,
        location=args.location,
        test_prompts=test_prompts
    )

    # ë¦¬í¬íŠ¸ ìƒì„±
    if results:
        generate_performance_report(results, args.output)
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
