#!/usr/bin/env python3
"""
Vertex AI Training Pipeline ì œì¶œ ìŠ¤í¬ë¦½íŠ¸
Consoleì—ì„œ ë³´ì´ëŠ” ê²ƒê³¼ ë™ì¼í•œ Training Job ìƒì„±
"""

import os
from google.cloud import aiplatform
from datetime import datetime

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ì„¤ì •
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/Users/choidamul/GCPmodel/.gcp-key.json"

# í”„ë¡œì íŠ¸ ì„¤ì •
PROJECT_ID = "knu-team-03"
LOCATION = "us-central1"  # A100 ì‚¬ìš© ê°€ëŠ¥ ë¦¬ì „
BUCKET_NAME = "knu-team-03-data"

# í•™ìŠµ ì„¤ì •
DISPLAY_NAME = f"gemma3-classical-lit-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

# Python íŒ¨í‚¤ì§€ ê²½ë¡œ
PYTHON_PACKAGE_URI = f"gs://{BUCKET_NAME}/vertex-training-packages/gemma_classical_trainer-1.0.0.tar.gz"
PYTHON_MODULE = "trainer.train"

# ë°ì´í„° ê²½ë¡œ (ì¦ê°• ë°ì´í„° ì‚¬ìš©)
TRAIN_DATA = f"gs://{BUCKET_NAME}/classical-literature/gemma/train_augmented.jsonl"
VALID_DATA = f"gs://{BUCKET_NAME}/classical-literature/gemma/valid_augmented.jsonl"
OUTPUT_DIR = f"gs://{BUCKET_NAME}/classical-literature/models/{DISPLAY_NAME}"

# HuggingFace í† í° (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
import os
HF_TOKEN = os.getenv("HUGGING_FACE_HUB_TOKEN", "")

# Container ì„¤ì • (PyTorch 2.4 + CUDA 12.1)
CONTAINER_URI = "us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-4.py310:latest"

# Vertex AI ì´ˆê¸°í™”
aiplatform.init(
    project=PROJECT_ID,
    location=LOCATION,
    staging_bucket=f"gs://{BUCKET_NAME}/staging"
)

print("=" * 60)
print("ğŸš€ Vertex AI Training Pipeline ì œì¶œ")
print("=" * 60)
print(f"í”„ë¡œì íŠ¸: {PROJECT_ID}")
print(f"ë¦¬ì „: {LOCATION}")
print(f"Job ì´ë¦„: {DISPLAY_NAME}")
print(f"íŒ¨í‚¤ì§€: {PYTHON_PACKAGE_URI}")
print(f"í•™ìŠµ ë°ì´í„°: {TRAIN_DATA}")
print(f"ì¶œë ¥: {OUTPUT_DIR}")
print("=" * 60)

# CustomPythonPackageTrainingJob ìƒì„±
job = aiplatform.CustomPythonPackageTrainingJob(
    display_name=DISPLAY_NAME,
    python_package_gcs_uri=PYTHON_PACKAGE_URI,
    python_module_name=PYTHON_MODULE,
    container_uri=CONTAINER_URI,
)

print("\nğŸ¯ Training Job ì‹œì‘ ì¤‘...")

# í•™ìŠµ ì¸ì
args = [
    "--train-data", TRAIN_DATA,
    "--valid-data", VALID_DATA,
    "--output-dir", OUTPUT_DIR,
    "--hf-token", HF_TOKEN,
    "--model-name", "google/gemma-2-9b-it",
    "--epochs", "3",
    "--batch-size", "4",
    "--grad-accum", "4",
    "--learning-rate", "2e-4",
    "--max-seq-length", "1024",
]

# Job ì‹¤í–‰ (A100 40GB)
model = job.run(
    args=args,
    replica_count=1,
    machine_type="a2-highgpu-1g",  # A100 40GB
    accelerator_type="NVIDIA_TESLA_A100",
    accelerator_count=1,
    base_output_dir=OUTPUT_DIR,
    sync=False,  # ë¹„ë™ê¸° ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)
)

print("\n" + "=" * 60)
print("âœ… Training Job ì œì¶œ ì™„ë£Œ!")
print("=" * 60)
print(f"Job ì´ë¦„: {DISPLAY_NAME}")
print(f"\nğŸ“Š ëª¨ë‹ˆí„°ë§:")
print(f"Console: https://console.cloud.google.com/vertex-ai/training/training-pipelines?project={PROJECT_ID}")
print("\nâ±ï¸ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1.5-2ì‹œê°„ (A100 40GB)")
print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
print("\nğŸ’¡ ìƒíƒœ í™•ì¸:")
print(f"gcloud ai custom-jobs list --region={LOCATION} --filter='displayName:{DISPLAY_NAME}'")
print("=" * 60)
