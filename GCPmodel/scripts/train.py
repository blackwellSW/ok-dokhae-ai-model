#!/usr/bin/env python3
"""
Gemma 3 íŒŒì¸íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model.trainer import GemmaTrainer, TrainingConfig


def main():
    parser = argparse.ArgumentParser(description="Gemma 3 íŒŒì¸íŠœë‹")

    # ë°ì´í„° ê²½ë¡œ
    parser.add_argument(
        "--train-data",
        type=str,
        default="",
        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ (JSONL)"
    )
    parser.add_argument(
        "--valid-data",
        type=str,
        default="",
        help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ (JSONL)"
    )

    # ëª¨ë¸ ì„¤ì •
    parser.add_argument(
        "--model-name",
        type=str,
        default="google/gemma-3-9b-it",
        help="ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="models/gemma_finetuned",
        help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
    )

    # í•™ìŠµ ì„¤ì •
    parser.add_argument(
        "--epochs",
        type=int,
        default=3,
        help="í•™ìŠµ ì—í­ ìˆ˜"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ"
    )
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=2e-4,
        help="í•™ìŠµë¥ "
    )
    parser.add_argument(
        "--max-length",
        type=int,
        default=1024,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )

    # LoRA ì„¤ì •
    parser.add_argument(
        "--lora-r",
        type=int,
        default=16,
        help="LoRA rank"
    )
    parser.add_argument(
        "--lora-alpha",
        type=int,
        default=32,
        help="LoRA alpha"
    )

    # ê¸°íƒ€
    parser.add_argument(
        "--config",
        type=str,
        default="",
        help="YAML ì„¤ì • íŒŒì¼ ê²½ë¡œ (ì„¤ì • íŒŒì¼ ì‚¬ìš© ì‹œ)"
    )
    parser.add_argument(
        "--resume",
        type=str,
        default="",
        help="ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘"
    )

    args = parser.parse_args()

    # ì„¤ì • íŒŒì¼ ë˜ëŠ” ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì‚¬ìš©
    if args.config:
        print(f"ğŸ“„ ì„¤ì • íŒŒì¼ ì‚¬ìš©: {args.config}")
        trainer = GemmaTrainer.from_yaml(args.config)
    else:
        config = TrainingConfig(
            model_name=args.model_name,
            train_data_path=args.train_data,
            valid_data_path=args.valid_data,
            output_dir=args.output_dir,
            num_train_epochs=args.epochs,
            per_device_train_batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            max_length=args.max_length,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha
        )
        trainer = GemmaTrainer(config)

    # ë°ì´í„° ê²½ë¡œ í™•ì¸
    if not trainer.config.train_data_path:
        print("=" * 60)
        print("âš ï¸ í•™ìŠµ ë°ì´í„° ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("=" * 60)
        print("\në‹¤ìŒ ë°©ë²• ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n")
        print("1. ì»¤ë§¨ë“œë¼ì¸ ì¸ì:")
        print("   python scripts/train.py --train-data data/processed/train.jsonl\n")
        print("2. ì„¤ì • íŒŒì¼:")
        print("   python scripts/train.py --config configs/training_config.yaml")
        print("   (training_config.yamlì—ì„œ data.train_data_path ì„¤ì •)\n")
        print("3. ë°ì´í„° ì¤€ë¹„:")
        print("   - AI HUBì—ì„œ ê³ ì „ë¬¸í•™ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
        print("   - python scripts/preprocess.py ì‹¤í–‰")
        print("=" * 60)
        return

    # í•™ìŠµ ì‹¤í–‰
    print("=" * 60)
    print("ğŸš€ Gemma 3 íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 60)
    print(f"   ëª¨ë¸: {trainer.config.model_name}")
    print(f"   í•™ìŠµ ë°ì´í„°: {trainer.config.train_data_path}")
    print(f"   ì¶œë ¥ ê²½ë¡œ: {trainer.config.output_dir}")
    print(f"   ì—í­: {trainer.config.num_train_epochs}")
    print(f"   ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {trainer.config.per_device_train_batch_size}")
    print(f"   í•™ìŠµë¥ : {trainer.config.learning_rate}")
    print("=" * 60)

    model_path = trainer.train(
        resume_from_checkpoint=args.resume if args.resume else None
    )

    if model_path:
        print("=" * 60)
        print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"   ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_path}")
        print("=" * 60)


if __name__ == "__main__":
    main()
