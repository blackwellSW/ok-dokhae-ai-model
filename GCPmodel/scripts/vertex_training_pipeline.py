#!/usr/bin/env python3
"""
Vertex AI Training Pipelineìœ¼ë¡œ Gemma 3 íŒŒì¸íŠœë‹
- ì™„ì „ ìë™í™”
- ë¸Œë¼ìš°ì € ë¶ˆí•„ìš”
- A100 GPU ì‚¬ìš©
"""

from google.cloud import aiplatform
from datetime import datetime

# í”„ë¡œì íŠ¸ ì„¤ì •
PROJECT_ID = "knu-team-03"
LOCATION = "us-central1"
BUCKET_NAME = "knu-team-03-data"

# í•™ìŠµ ì„¤ì •
DISPLAY_NAME = f"gemma3-classical-lit-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
CONTAINER_URI = "us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.2-4.py310:latest"

# ë°ì´í„° ê²½ë¡œ
TRAIN_DATA = f"gs://{BUCKET_NAME}/classical-literature/gemma/train_balanced.jsonl"
VALID_DATA = f"gs://{BUCKET_NAME}/classical-literature/gemma/valid_balanced.jsonl"
OUTPUT_DIR = f"gs://{BUCKET_NAME}/classical-literature/models/{DISPLAY_NAME}"

# HuggingFace í† í°
HF_TOKEN = "YOUR_HF_TOKEN_HERE"

# Vertex AI ì´ˆê¸°í™”
aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=f"gs://{BUCKET_NAME}/staging")

print("="*60)
print("ğŸš€ Vertex AI Training Pipeline ì‹œì‘")
print("="*60)
print(f"í”„ë¡œì íŠ¸: {PROJECT_ID}")
print(f"ë¦¬ì „: {LOCATION}")
print(f"Job ì´ë¦„: {DISPLAY_NAME}")
print(f"í•™ìŠµ ë°ì´í„°: {TRAIN_DATA}")
print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
print("="*60)

# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë¨)
TRAINING_SCRIPT = """
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset
from trl import SFTTrainer
from huggingface_hub import login
import sys

# HuggingFace ë¡œê·¸ì¸
login(token="{hf_token}")

# ë°ì´í„° ë¡œë“œ
print("ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘...")
train_dataset = load_dataset('json', data_files='{train_data}', split='train')
valid_dataset = load_dataset('json', data_files='{valid_data}', split='train')
print(f"âœ… Train: {len(train_dataset)}, Valid: {len(valid_dataset)}")

# ëª¨ë¸ ë¡œë“œ
print("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
model_name = "google/gemma-2-9b-it"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    attn_implementation="eager"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# LoRA ì„¤ì •
print("ğŸ”§ LoRA ì„¤ì • ì¤‘...")
peft_config = LoraConfig(
    lora_alpha=32,
    lora_dropout=0.1,
    r=16,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# ë°ì´í„° í¬ë§·íŒ…
def formatting_func(examples):
    texts = []
    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):
        text = f\"\"\"<start_of_turn>user
{{instruction}}

{{input_text}}<end_of_turn>
<start_of_turn>model
{{output}}<end_of_turn>\"\"\"
        texts.append(text)
    return texts

# Trainer ì„¤ì • (A100 ìµœì í™”)
print("âš™ï¸ Trainer ì„¤ì • ì¤‘...")
training_args = TrainingArguments(
    output_dir="{output_dir}",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=10,
    eval_strategy="steps",
    eval_steps=50,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=1,
    bf16=True,
    tf32=True,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={{"use_reentrant": False}},
    max_grad_norm=1.0,
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    report_to="none",
    load_best_model_at_end=False,
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    formatting_func=formatting_func,
    packing=False,
    max_seq_length=1024,
    dataset_text_field=None,
    tokenizer=tokenizer,
)

# í•™ìŠµ ì‹œì‘
print("="*60)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("="*60)
trainer.train()

# ëª¨ë¸ ì €ì¥
print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
trainer.save_model("{output_dir}")
tokenizer.save_pretrained("{output_dir}")
print("âœ… í•™ìŠµ ì™„ë£Œ!")
"""

# ìŠ¤í¬ë¦½íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (ì»¨í…Œì´ë„ˆì—ì„œ ì‹¤í–‰)
script_content = TRAINING_SCRIPT.format(
    hf_token=HF_TOKEN,
    train_data=TRAIN_DATA,
    valid_data=VALID_DATA,
    output_dir=OUTPUT_DIR
)

# GCSì— ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ
import tempfile
import os
from google.cloud import storage

with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
    f.write(script_content)
    script_path = f.name

# Upload to GCS
storage_client = storage.Client(project=PROJECT_ID)
bucket = storage_client.bucket(BUCKET_NAME)
blob = bucket.blob(f"staging/train_script_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py")
blob.upload_from_filename(script_path)
script_uri = f"gs://{BUCKET_NAME}/{blob.name}"
os.unlink(script_path)

print(f"ğŸ“¤ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì—…ë¡œë“œ: {script_uri}")

# Custom Job ìƒì„±
job = aiplatform.CustomPythonPackageTrainingJob(
    display_name=DISPLAY_NAME,
    python_package_gcs_uri=script_uri,
    python_module_name="train_script",
    container_uri=CONTAINER_URI,
)

print("\nğŸ¯ Training Job ì‹œì‘ ì¤‘...")

# Job ì‹¤í–‰
model = job.run(
    replica_count=1,
    machine_type="a2-highgpu-1g",  # A100 40GB
    accelerator_type="NVIDIA_TESLA_A100",
    accelerator_count=1,
    base_output_dir=OUTPUT_DIR,
    sync=True,  # ë™ê¸° ì‹¤í–‰ (ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
)

print("\n" + "="*60)
print("âœ… Training Pipeline ì™„ë£Œ!")
print("="*60)
print(f"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
print(f"\nëª¨ë‹ˆí„°ë§:")
print(f"https://console.cloud.google.com/vertex-ai/training/training-pipelines?project={PROJECT_ID}")
print("="*60)
