#!/usr/bin/env python3
"""
ë°°í¬ëœ Gemma 2 9B vLLM ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
ì§ì ‘ REST APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸ì¦ ë¬¸ì œ ìš°íšŒ
"""

import json
import subprocess
import requests


def get_access_token():
    """gcloudì—ì„œ ì•¡ì„¸ìŠ¤ í† í° ê°€ì ¸ì˜¤ê¸°"""
    try:
        token = subprocess.check_output(
            ["gcloud", "auth", "print-access-token"],
            stderr=subprocess.PIPE
        ).decode("utf-8").strip()
        return token
    except subprocess.CalledProcessError as e:
        print(f"âŒ í† í° íšë“ ì‹¤íŒ¨: {e}")
        print(f"stderr: {e.stderr.decode('utf-8')}")
        return None


def test_vllm_endpoint(
    endpoint_id: str,
    project: str = "knu-team-03",
    region: str = "us-central1"
):
    """vLLM ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸"""

    print("=" * 70)
    print("ğŸš€ Gemma 2 9B + vLLM + LoRA ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 70)
    print(f"Project: {project}")
    print(f"Region: {region}")
    print(f"Endpoint ID: {endpoint_id}")
    print("=" * 70)

    # ì•¡ì„¸ìŠ¤ í† í° íšë“
    token = get_access_token()
    if not token:
        print("âš ï¸ ë¨¼ì € 'gcloud auth login'ì„ ì‹¤í–‰í•˜ì„¸ìš”")
        return

    # API URL
    url = f"https://{region}-aiplatform.googleapis.com/v1/projects/{project}/locations/{region}/endpoints/{endpoint_id}:rawPredict"

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }

    # ê³ ì „ ë¬¸í•™ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "name": "ì¶˜í–¥ì „ - ì£¼ì¸ê³µ ì§ˆë¬¸",
            "question": "ì¶˜í–¥ì „ì˜ ì£¼ì¸ê³µì€ ëˆ„êµ¬ì¸ê°€ìš”?",
            "context": "ì¶˜í–¥ì „"
        },
        {
            "name": "ì¶˜í–¥ì „ - ì´ëª½ë£¡ ì‹ ë¶„",
            "question": "ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ê°€ ë­”ê°€ìš”?",
            "context": "ì¶˜í–¥ì „"
        },
        {
            "name": "ì‹¬ì²­ì „ - ì¸ë‹¹ìˆ˜",
            "question": "ì‹¬ì²­ì´ëŠ” ì™œ ì¸ë‹¹ìˆ˜ì— ëª¸ì„ ë˜ì¡Œë‚˜ìš”?",
            "context": "ì‹¬ì²­ì „"
        },
        {
            "name": "í¥ë¶€ì „ - í˜•ì œ ë¹„êµ",
            "question": "í¥ë¶€ì™€ ë†€ë¶€ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "context": "í¥ë¶€ì „"
        },
        {
            "name": "ì¼ë°˜ í•œêµ­ì–´ ëŠ¥ë ¥",
            "question": "ì¡°ì„ ì‹œëŒ€ ê³¼ê±°ì œë„ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "context": "í•œêµ­ì‚¬"
        }
    ]

    success_count = 0
    total = len(test_cases)

    for i, tc in enumerate(test_cases, 1):
        print(f"\n{'=' * 70}")
        print(f"[í…ŒìŠ¤íŠ¸ {i}/{total}] {tc['name']}")
        print("=" * 70)
        print(f"ğŸ“š ë¬¸ë§¥: {tc['context']}")
        print(f"ğŸ’¬ ì§ˆë¬¸: {tc['question']}")
        print("-" * 70)

        # vLLM OpenAI-compatible API í¬ë§· (system role ë¯¸ì§€ì›)
        # LoRA ì–´ëŒ‘í„°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ modelì„ "classical-lit"ë¡œ ì§€ì •
        user_prompt = f"ë‹¹ì‹ ì€ {tc['context']} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  êµìœ¡ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {tc['question']}\n\në‹µë³€:"

        payload = {
            "model": "classical-lit",  # vLLM --lora-modulesì—ì„œ ì •ì˜í•œ ì´ë¦„
            "messages": [
                {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            "max_tokens": 256,
            "temperature": 0.7,
            "top_p": 0.9
        }

        try:
            # API í˜¸ì¶œ
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=60
            )

            # ì‘ë‹µ í™•ì¸
            if response.status_code == 200:
                result = response.json()

                # vLLMì€ OpenAI í¬ë§·ìœ¼ë¡œ ì‘ë‹µ
                choices = result.get("choices", [])
                if choices:
                    answer = choices[0].get("message", {}).get("content", "")
                    usage = result.get("usage", {})

                    print(f"\nğŸ¤– ì‘ë‹µ:")
                    print(f"{answer}")
                    print(f"\nğŸ“Š í† í° ì‚¬ìš©:")
                    print(f"  - ì…ë ¥: {usage.get('prompt_tokens', 'N/A')}")
                    print(f"  - ì¶œë ¥: {usage.get('completion_tokens', 'N/A')}")
                    print(f"  - ì´: {usage.get('total_tokens', 'N/A')}")

                    success_count += 1
                else:
                    print(f"âš ï¸ ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
                    print(f"Raw response: {json.dumps(result, indent=2, ensure_ascii=False)}")
            else:
                print(f"âŒ HTTP {response.status_code}")
                print(f"Response: {response.text}")

        except requests.exceptions.Timeout:
            print(f"â±ï¸ íƒ€ì„ì•„ì›ƒ (60ì´ˆ ì´ˆê³¼)")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")

    # ìµœì¢… ê²°ê³¼
    print(f"\n{'=' * 70}")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {success_count}/{total} ì„±ê³µ")
    print("=" * 70)

    if success_count == total:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("âœ¨ LoRA ì–´ëŒ‘í„°ê°€ ì •ìƒ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
    elif success_count > 0:
        print(f"\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ({total - success_count}ê°œ)")
    else:
        print("\nâŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")


if __name__ == "__main__":
    # deployment_info.jsonì—ì„œ ì •ë³´ ë¡œë“œ
    import os

    deployment_info_path = "/Users/choidamul/GCPmodel/deployment_info.json"

    if os.path.exists(deployment_info_path):
        with open(deployment_info_path) as f:
            info = json.load(f)

        test_vllm_endpoint(
            endpoint_id=info["endpoint_id"],
            project=info["project_id"],
            region=info["region"]
        )
    else:
        print("âŒ deployment_info.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
