#!/usr/bin/env python3
"""
Gemma ì‚¬ê³ ìœ ë„ ëª¨ë¸ ë°ëª¨ (Streamlit)

ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ê¸°ë°˜ ì‘ë‹µ ì‹œì—°
ì‹¤ì œ ëª¨ë“œ: Vertex AI Endpoint í˜¸ì¶œ (ë°°í¬ í•„ìš”)
"""

import streamlit as st
import json
import random
import subprocess
import requests
from pathlib import Path

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê³ ì „ë¬¸í•™ ì‚¬ê³ ìœ ë„ AI êµì‚¬",
    page_icon="ğŸ“š",
    layout="wide"
)

# íƒ€ì´í‹€
st.title("ğŸ“š ê³ ì „ë¬¸í•™ ì‚¬ê³ ìœ ë„ AI êµì‚¬")
st.markdown("**Gemma 2 9B-IT + LoRA** ê¸°ë°˜ ì†Œí¬ë¼í‹± ëŒ€í™” ëª¨ë¸")

# ì‚¬ì´ë“œë°” - ëª¨ë“œ ì„ íƒ
st.sidebar.header("âš™ï¸ ì„¤ì •")
mode = st.sidebar.radio(
    "ì‹¤í–‰ ëª¨ë“œ",
    ["ì‹œë®¬ë ˆì´ì…˜ (í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ)", "ì‹¤ì œ ì¶”ë¡  (Endpoint í•„ìš”)"],
    index=0
)

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“Š ëª¨ë¸ ì •ë³´")
st.sidebar.markdown("""
- **ë² ì´ìŠ¤ ëª¨ë¸**: Gemma 2 9B-IT
- **í•™ìŠµ ë°©ë²•**: LoRA (r=16, Î±=32)
- **í•™ìŠµ ë°ì´í„°**: 3,056ê°œ ìƒ˜í”Œ
- **íƒœê·¸ ì‚¬ìš©ë¥ **: 80%
- **í‰ê·  ì§ˆë¬¸ ìˆ˜**: 19.7ê°œ
""")


# ============================================================
# ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
# ============================================================

def load_training_samples():
    """í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ë¡œë“œ"""
    train_file = Path(__file__).parent.parent / "data/final/train_tagged.jsonl"

    if not train_file.exists():
        return []

    samples = []
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            samples.append(json.loads(line))

    return samples


def display_simulation_mode():
    """ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ UI"""
    st.markdown("### ğŸ­ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ")
    st.info("í•™ìŠµ ë°ì´í„°ì˜ ì‹¤ì œ ìƒ˜í”Œì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµí•œ ì‘ë‹µ íŒ¨í„´ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    samples = load_training_samples()

    if not samples:
        st.error("í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: data/final/train_tagged.jsonl")
        return

    # ì‘í’ˆ í•„í„°
    contexts = list(set(s.get("instruction", "").split("[ì‘í’ˆ: ")[-1].split("]")[0]
                       for s in samples if "[ì‘í’ˆ:" in s.get("instruction", "")))
    contexts = [c for c in contexts if c][:10]  # ìƒìœ„ 10ê°œ

    col1, col2 = st.columns([2, 1])

    with col1:
        selected_context = st.selectbox(
            "ì‘í’ˆ ì„ íƒ",
            ["ì „ì²´"] + sorted(contexts),
            index=0
        )

    with col2:
        if st.button("ğŸ² ëœë¤ ìƒ˜í”Œ", use_container_width=True):
            st.session_state.random_seed = random.randint(0, len(samples) - 1)

    # í•„í„°ë§
    if selected_context != "ì „ì²´":
        filtered = [s for s in samples if f"[ì‘í’ˆ: {selected_context}]" in s.get("instruction", "")]
    else:
        filtered = samples

    if not filtered:
        st.warning("í•´ë‹¹ ì‘í’ˆì˜ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ìƒ˜í”Œ ì„ íƒ
    if "random_seed" in st.session_state:
        sample_idx = st.session_state.random_seed % len(filtered)
    else:
        sample_idx = 0

    sample = filtered[sample_idx]

    st.markdown("---")

    # ì…ë ¥ í‘œì‹œ
    st.markdown("### ğŸ’¬ í•™ìƒ ì§ˆë¬¸")

    instruction = sample.get("instruction", "")
    student_input = sample.get("input", "")

    # ì‘í’ˆ ì¶”ì¶œ
    if "[ì‘í’ˆ:" in instruction:
        work = instruction.split("[ì‘í’ˆ: ")[-1].split("]")[0]
        st.markdown(f"**ì‘í’ˆ**: {work}")

    st.markdown(f"**ì§ˆë¬¸**: {student_input}")

    # ì‘ë‹µ í‘œì‹œ
    st.markdown("### ğŸ¤– AI êµì‚¬ ì‘ë‹µ")

    output = sample.get("output", "")

    # íƒœê·¸ ë¶„ì„
    has_induction = "[ì‚¬ê³ ìœ ë„]" in output
    has_log = "[ì‚¬ê³ ë¡œê·¸]" in output
    question_count = output.count("?") + output.count("ê¹Œìš”")

    # ì‘ë‹µ ì¶œë ¥
    response_container = st.container()
    with response_container:
        st.markdown(output)

    # ë¶„ì„ í‘œì‹œ
    st.markdown("---")
    st.markdown("### ğŸ“Š ì‘ë‹µ ë¶„ì„")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        if has_induction:
            st.success("âœ… [ì‚¬ê³ ìœ ë„] íƒœê·¸")
        else:
            st.error("âŒ [ì‚¬ê³ ìœ ë„] íƒœê·¸")

    with col2:
        if has_log:
            st.success("âœ… [ì‚¬ê³ ë¡œê·¸] íƒœê·¸")
        else:
            st.error("âŒ [ì‚¬ê³ ë¡œê·¸] íƒœê·¸")

    with col3:
        st.metric("ì§ˆë¬¸ ìˆ˜", f"{question_count}ê°œ")

    with col4:
        is_evaluator = any(kw in output for kw in ["ì „ë‹¬í•˜ì§€ ëª»í•˜ê³ ", "ë¶€ì¡±í•˜ì—¬", "í‰ê°€", "ì ìˆ˜"])
        if is_evaluator:
            st.error("âŒ í‰ê°€ì ëª¨ë“œ")
        else:
            st.success("âœ… ì‚¬ê³ ìœ ë„ ëª¨ë“œ")

    # ìƒ˜í”Œ ì •ë³´
    st.caption(f"ìƒ˜í”Œ {sample_idx + 1} / {len(filtered)}")


# ============================================================
# ì‹¤ì œ ì¶”ë¡  ëª¨ë“œ
# ============================================================

@st.cache_data(ttl=300)
def get_access_token():
    """GCP ì•¡ì„¸ìŠ¤ í† í° ê°€ì ¸ì˜¤ê¸° (5ë¶„ ìºì‹±)"""
    try:
        token = subprocess.check_output(
            ["gcloud", "auth", "print-access-token"],
            stderr=subprocess.PIPE
        ).decode("utf-8").strip()
        return token
    except Exception as e:
        return None


def call_vertex_ai_endpoint(endpoint_id, project, region, question, context, max_tokens=512, temperature=0.7):
    """Vertex AI vLLM ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ"""
    token = get_access_token()
    if not token:
        return {"error": "gcloud ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. 'gcloud auth login'ì„ ì‹¤í–‰í•˜ì„¸ìš”."}

    # API URL
    url = f"https://{region}-aiplatform.googleapis.com/v1/projects/{project}/locations/{region}/endpoints/{endpoint_id}:rawPredict"

    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }

    # vLLM OpenAI-compatible API í¬ë§·
    user_prompt = f"ë‹¹ì‹ ì€ {context} ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•™ìƒì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  êµìœ¡ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"

    payload = {
        "model": "classical-lit",  # LoRA ëª¨ë“ˆ ì´ë¦„
        "messages": [
            {
                "role": "user",
                "content": user_prompt
            }
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9
    }

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=180)

        if response.status_code == 200:
            result = response.json()
            choices = result.get("choices", [])

            if choices:
                answer = choices[0].get("message", {}).get("content", "")
                usage = result.get("usage", {})

                return {
                    "success": True,
                    "answer": answer,
                    "usage": usage
                }
            else:
                return {"error": "ì‘ë‹µ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
        else:
            return {"error": f"HTTP {response.status_code}: {response.text}"}

    except requests.exceptions.Timeout:
        return {"error": "ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (180ì´ˆ). ì—”ë“œí¬ì¸íŠ¸ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}
    except Exception as e:
        return {"error": str(e)}


def display_inference_mode():
    """ì‹¤ì œ ì¶”ë¡  ëª¨ë“œ UI"""
    st.markdown("### ğŸš€ ì‹¤ì œ ì¶”ë¡  ëª¨ë“œ")

    # deployment_info.json ìë™ ë¡œë“œ
    deployment_info_path = Path(__file__).parent.parent / "deployment_info.json"
    auto_loaded = False

    if deployment_info_path.exists():
        with open(deployment_info_path) as f:
            deployment_info = json.load(f)

        st.success(f"âœ… ë°°í¬ ì •ë³´ ìë™ ë¡œë“œë¨ (ë°°í¬ì¼: {deployment_info.get('deployed_at', 'N/A')})")

        endpoint_id = deployment_info.get("endpoint_id", "")
        project = deployment_info.get("project_id", "knu-team-03")
        region = deployment_info.get("region", "us-central1")
        auto_loaded = True

    else:
        st.warning("âš ï¸ deployment_info.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.")

        col1, col2 = st.columns(2)
        with col1:
            project = st.text_input("Project ID", value="knu-team-03")
        with col2:
            region = st.text_input("Region", value="us-central1")

        endpoint_id = st.text_input(
            "Endpoint ID",
            placeholder="2283851677146546176",
            help="ìˆ«ìë§Œ ì…ë ¥ (ì „ì²´ ë¦¬ì†ŒìŠ¤ ì´ë¦„ ë¶ˆí•„ìš”)"
        )

    if not endpoint_id:
        st.info("Endpoint IDë¥¼ ì…ë ¥í•˜ë©´ ì‹¤ì œ ëª¨ë¸ê³¼ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    st.markdown("---")

    # ì…ë ¥ í¼
    col1, col2 = st.columns([1, 3])

    with col1:
        context = st.selectbox(
            "ì‘í’ˆ ì„ íƒ",
            ["ì¶˜í–¥ì „", "ì‹¬ì²­ì „", "í¥ë¶€ì „", "í™ê¸¸ë™ì „", "êµ¬ìš´ëª½", "ì‚¬ì”¨ë‚¨ì •ê¸°", "í•œêµ­ì‚¬", "ê¸°íƒ€"],
            index=0
        )

        if context == "ê¸°íƒ€":
            context = st.text_input("ì‘í’ˆëª… ì…ë ¥")

        max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", 128, 1024, 512, 128)
        temperature = st.slider("Temperature", 0.0, 1.0, 0.7, 0.1)

    with col2:
        student_question = st.text_area(
            "í•™ìƒ ì§ˆë¬¸",
            placeholder="ì˜ˆ: ì¶˜í–¥ì „ì—ì„œ ì´ëª½ë£¡ì´ ì‹ ë¶„ì„ ìˆ¨ê¸´ ì´ìœ ê°€ ë­”ê°€ìš”?",
            height=150
        )

    if st.button("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", use_container_width=True, type="primary"):
        if not student_question:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

        with st.spinner("ğŸ¤” AI êµì‚¬ê°€ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            result = call_vertex_ai_endpoint(
                endpoint_id=endpoint_id,
                project=project,
                region=region,
                question=student_question,
                context=context,
                max_tokens=max_tokens,
                temperature=temperature
            )

        if result.get("success"):
            st.markdown("---")
            st.markdown("### ğŸ¤– AI êµì‚¬ ì‘ë‹µ")

            answer = result["answer"]
            st.markdown(answer)

            # ë¶„ì„
            st.markdown("---")
            st.markdown("### ğŸ“Š ì‘ë‹µ ë¶„ì„")

            col1, col2, col3, col4 = st.columns(4)

            has_induction = "[ì‚¬ê³ ìœ ë„]" in answer
            has_log = "[ì‚¬ê³ ë¡œê·¸]" in answer
            question_count = answer.count("?") + answer.count("ê¹Œìš”")

            with col1:
                if has_induction:
                    st.success("âœ… [ì‚¬ê³ ìœ ë„]")
                else:
                    st.error("âŒ [ì‚¬ê³ ìœ ë„]")

            with col2:
                if has_log:
                    st.success("âœ… [ì‚¬ê³ ë¡œê·¸]")
                else:
                    st.error("âŒ [ì‚¬ê³ ë¡œê·¸]")

            with col3:
                st.metric("ì§ˆë¬¸ ìˆ˜", f"{question_count}ê°œ")

            with col4:
                usage = result.get("usage", {})
                total_tokens = usage.get("total_tokens", 0)
                st.metric("ì´ í† í°", total_tokens)

            # í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸
            with st.expander("ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸"):
                st.json(usage)

        else:
            st.error(f"âŒ ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            st.info("gcloud ì¸ì¦ì„ í™•ì¸í•˜ì„¸ìš”: `gcloud auth login`")


# ============================================================
# ë©”ì¸
# ============================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    # ëª¨ë“œë³„ UI í‘œì‹œ
    if "ì‹œë®¬ë ˆì´ì…˜" in mode:
        display_simulation_mode()
    else:
        display_inference_mode()

    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: gray;'>
        <p>ê³ ì „ë¬¸í•™ ì‚¬ê³ ìœ ë„ AI êµì‚¬ | Gemma 2 9B-IT + LoRA</p>
        <p>KNU Team 03 | 2026-02</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
